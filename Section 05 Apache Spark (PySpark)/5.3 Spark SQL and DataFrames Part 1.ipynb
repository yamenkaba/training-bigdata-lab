{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Built-in Data Sources\n",
    "Spark SQL:\n",
    "- Provides the engine upon which the high-level Structured APIs we explored previously are built.\n",
    "- Can read and write data in a variety of structured formats (e.g., JSON, Hive tables, Parquet, Avro, ORC, CSV).\n",
    "- Lets you query data using JDBC/ODBC connectors from external business intelligence (BI) data sources such as Tableau, Power BI, Talend, or from RDBMSs such as MySQL and PostgreSQL.\n",
    "- Offers an interactive shell to issue SQL queries on your structured data.\n",
    "- Supports ANSI SQL:2003-compliant commands and HiveQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Spark SQL in Spark Applications\n",
    "The SparkSession, introduced in Spark 2.0, provides a unified entry point for programming Spark with the Structured APIs. You can use a SparkSession to access Spark functionality.\n",
    "To issue any SQL query, use the `sql()` method on the `SparkSession` instance, spark, such as `spark.sql(\"SELECT * FROM tableName\")`. All  `spark.sql` queries executed in this manner return a DataFrame on which you may perform further Spark operations if you desire.\n",
    "\n",
    "### Basic Query Examples\n",
    "The U.S. Department of Transportation's (DOT) Bureau of Transportation Statistics (BTS) tracks the on-time performance of domestic flights operated by large air carriers. Summary information on the number of on-time, delayed, canceled and diverted flights appears in DOT's monthly Air Travel Consumer Report, published about 30 days after the month's end, as well as in summary tables posted on this website. BTS began collecting details on the causes of flight delays in June 2003. Summary statistics and raw data are made available to the public at the time the Air Travel Consumer Report is released.\n",
    "Data is available as a CSV file with over a million records. Using a schema, we’ll read the data into a DataFrame and register\n",
    "the DataFrame as a temporary view so we can query it with SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "# If you know spark path you can specify it as init function parameter\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = (SparkSession\n",
    "            .builder\n",
    "            .appName(\"SparkSQLandDFsPart1\")\n",
    "            .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data set\n",
    "csv_file = \"../data/DelayedFlights.csv\"\n",
    "\n",
    "# Read and create a temporary view\n",
    "# Infer schema (note that for larger files you\n",
    "# may want to specify the schema)\n",
    "df = (spark.read.format(\"csv\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(csv_file))\n",
    "\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note!</b> If you want to specify a schema, you can use a DDL-formatted string. For example:\n",
    "\n",
    "`schema = \"date STRING, delay INT, distance INT, origin STRING, destination STRING\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a temporary view, we can issue SQL queries using Spark SQL. These queries are no different from those you might issue against a SQL table (ex: MySQL or PostgreSQL) database. The point here is to show that Spark SQL offers an `ANSI:2003–compliant` SQL interface, and to demonstrate the interoperability between SQL and DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: double (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- ArrTime: double (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- ActualElapsedTime: double (nullable = true)\n",
      " |-- CRSElapsedTime: double (nullable = true)\n",
      " |-- AirTime: double (nullable = true)\n",
      " |-- ArrDelay: double (nullable = true)\n",
      " |-- DepDelay: double (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      " |-- TaxiIn: double (nullable = true)\n",
      " |-- TaxiOut: double (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: integer (nullable = true)\n",
      " |-- CarrierDelay: double (nullable = true)\n",
      " |-- WeatherDelay: double (nullable = true)\n",
      " |-- NASDelay: double (nullable = true)\n",
      " |-- SecurityDelay: double (nullable = true)\n",
      " |-- LateAircraftDelay: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0',\n",
       " 'Year',\n",
       " 'Month',\n",
       " 'DayofMonth',\n",
       " 'DayOfWeek',\n",
       " 'DepTime',\n",
       " 'CRSDepTime',\n",
       " 'ArrTime',\n",
       " 'CRSArrTime',\n",
       " 'UniqueCarrier',\n",
       " 'FlightNum',\n",
       " 'TailNum',\n",
       " 'ActualElapsedTime',\n",
       " 'CRSElapsedTime',\n",
       " 'AirTime',\n",
       " 'ArrDelay',\n",
       " 'DepDelay',\n",
       " 'Origin',\n",
       " 'Dest',\n",
       " 'Distance',\n",
       " 'TaxiIn',\n",
       " 'TaxiOut',\n",
       " 'Cancelled',\n",
       " 'CancellationCode',\n",
       " 'Diverted',\n",
       " 'CarrierDelay',\n",
       " 'WeatherDelay',\n",
       " 'NASDelay',\n",
       " 'SecurityDelay',\n",
       " 'LateAircraftDelay']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let’s try some example queries against this data set. First, we’ll find all flights whose distance is greater than 1,000 miles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    1003|   DFW|        FNT|\n",
      "|    1003|   DFW|        FNT|\n",
      "|    1003|   DFW|        FNT|\n",
      "|    1003|   DFW|        FNT|\n",
      "|    1003|   DFW|        FNT|\n",
      "|    1003|   DFW|        FNT|\n",
      "|    1003|   DFW|        FNT|\n",
      "|    1003|   DFW|        FNT|\n",
      "|    1003|   DFW|        FNT|\n",
      "|    1003|   DFW|        FNT|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT Distance as distance, Origin as origin, Dest as destination\n",
    "             FROM us_delay_flights_tbl WHERE Distance > 1000\n",
    "             ORDER BY Distance DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the results show, all of the longest flights were between (DFW) and (FNT). Next, we’ll find all flights between San Francisco (SFO) and Chicago (ORD) with at least a two-hour delay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+-----------+\n",
      "|      date| delay|origin|destination|\n",
      "+----------+------+------+-----------+\n",
      "| 30/9/2008|1032.0|   SFO|        ORD|\n",
      "|14/10/2008| 841.0|   SFO|        ORD|\n",
      "| 27/7/2008| 831.0|   SFO|        ORD|\n",
      "|  4/1/2008| 779.0|   SFO|        ORD|\n",
      "| 30/9/2008| 759.0|   SFO|        ORD|\n",
      "| 31/1/2008| 733.0|   SFO|        ORD|\n",
      "| 15/2/2008| 707.0|   SFO|        ORD|\n",
      "| 15/7/2008| 671.0|   SFO|        ORD|\n",
      "| 17/2/2008| 669.0|   SFO|        ORD|\n",
      "| 31/1/2008| 668.0|   SFO|        ORD|\n",
      "+----------+------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT CONCAT(dayOfMonth, '/', month, '/', year) as date, (ArrDelay + DepDelay) as delay, Origin as origin, Dest as destination\n",
    "             FROM us_delay_flights_tbl\n",
    "             WHERE (Origin = 'SFO') AND (Dest = 'ORD')\n",
    "             ORDER by delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the DataFrame and Dataset APIs, with the `spark.sql` interface you can conduct common data analysis operations like those we explored in the previous chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three of the preceding SQL queries can be expressed with an equivalent Data‐Frame API query. For example, the first query can be expressed in the Python Data‐Frame API as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+----+\n",
      "|ArrDelay|DepDelay|Origin|Dest|\n",
      "+--------+--------+------+----+\n",
      "|     9.0|     6.0|   EWR| HNL|\n",
      "|    99.0|    84.0|   HNL| EWR|\n",
      "|    14.0|    33.0|   HNL| EWR|\n",
      "|    48.0|    42.0|   EWR| HNL|\n",
      "|   -12.0|    11.0|   EWR| HNL|\n",
      "|    29.0|    61.0|   EWR| HNL|\n",
      "|     1.0|     8.0|   EWR| HNL|\n",
      "|   264.0|   228.0|   HNL| EWR|\n",
      "|    -7.0|    19.0|   EWR| HNL|\n",
      "|    -5.0|    10.0|   EWR| HNL|\n",
      "+--------+--------+------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In Python\n",
    "from pyspark.sql.functions import col, desc\n",
    "\n",
    "(df.select(\"ArrDelay\", \"DepDelay\", \"Origin\", \"Dest\")\n",
    "    .where(col(\"distance\") > 1000)\n",
    "    .orderBy(desc(\"distance\"))).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+----+\n",
      "|ArrDelay|DepDelay|Origin|Dest|\n",
      "+--------+--------+------+----+\n",
      "|     9.0|     6.0|   EWR| HNL|\n",
      "|    99.0|    84.0|   HNL| EWR|\n",
      "|    14.0|    33.0|   HNL| EWR|\n",
      "|    48.0|    42.0|   EWR| HNL|\n",
      "|   -12.0|    11.0|   EWR| HNL|\n",
      "|    29.0|    61.0|   EWR| HNL|\n",
      "|     1.0|     8.0|   EWR| HNL|\n",
      "|   264.0|   228.0|   HNL| EWR|\n",
      "|    -7.0|    19.0|   EWR| HNL|\n",
      "|    -5.0|    10.0|   EWR| HNL|\n",
      "+--------+--------+------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Or\n",
    "(df.select(\"ArrDelay\", \"DepDelay\", \"Origin\", \"Dest\")\n",
    "    .where(\"distance > 1000\")\n",
    "    .orderBy(\"distance\", ascending=False).show(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable you to query structured data as shown in the preceding examples, Spark manages all the complexities of creating and managing views and tables, both in memory and on disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Tables and Views\n",
    "Tables hold data. Associated with each table in Spark is its relevant metadata, which is information about the table and its data: the schema, description, table name, database name, column names, partitions, physical location where the actual data resides, etc. All of this is stored in a central metastore.\n",
    "Instead of having a separate metastore for Spark tables, Spark by default uses the Apache Hive metastore, located at `/user/hive/warehouse`, to persist all the metadata about your tables. However, you may change the default location by setting the Spark config variable `spark.sql.warehouse.dir` to another location, which can be set to a local or external distributed storage.\n",
    "\n",
    "### Managed Versus UnmanagedTables\n",
    "Spark allows you to create two types of tables: managed and unmanaged. For a `managed` table, Spark manages both the metadata and the data in the file store. This could\n",
    "be a local filesystem, HDFS, or an object store such as Amazon S3 or Azure Blob. For an `unmanaged` table, Spark only manages the metadata, while you manage the data\n",
    "yourself in an external data source such as Cassandra.\n",
    "With a managed table, because Spark manages everything, a SQL command such as DROP TABLE `table_name` deletes both the metadata and the data. With an unmanaged table, the same command will delete only the metadata, not the actual data. We will look at some examples of how to create managed and unmanaged tables in the next section.\n",
    "\n",
    "### Creating SQL Databases and Tables\n",
    "By default, Spark creates tables under the default database. To create your own database name, you can issue a SQL command fromyour Spark application or notebook. Using the US flight delays data set, let’s create both a managed and an unmanaged table. To begin, we’ll create a database called `learn_spark_db` and tell Spark we want to use that database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE learn_spark_db\")\n",
    "spark.sql(\"USE learn_spark_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point, any commands we issue in our application to create tables will result in the tables being created in this database and residing under the database name `learn_spark_db`.\n",
    "\n",
    "#### Creating a managed table\n",
    "To create a managed table within the database learn_spark_db, you can issue a SQL query like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Hive support is required to CREATE Hive TABLE (AS SELECT);\n'CreateTable `learn_spark_db`.`managed_us_delay_flights_tbl`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [45], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Using SQL\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m spark\u001b[39m.\u001b[39;49msql(\u001b[39m\"\"\"\u001b[39;49m\u001b[39mCREATE TABLE managed_us_delay_flights_tbl (date STRING, delay INT,\u001b[39;49m\n\u001b[0;32m      3\u001b[0m \u001b[39m             distance INT, origin STRING, destination STRING)\u001b[39;49m\u001b[39m\"\"\"\u001b[39;49m)\n",
      "File \u001b[1;32m~\\spark-3.3.1\\python\\pyspark\\sql\\session.py:1034\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[1;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[0;32m   1032\u001b[0m     sqlQuery \u001b[39m=\u001b[39m formatter\u001b[39m.\u001b[39mformat(sqlQuery, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1033\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1034\u001b[0m     \u001b[39mreturn\u001b[39;00m DataFrame(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsparkSession\u001b[39m.\u001b[39;49msql(sqlQuery), \u001b[39mself\u001b[39m)\n\u001b[0;32m   1035\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   1036\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(kwargs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32m~\\spark-3.3.1\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\spark-3.3.1\\python\\pyspark\\sql\\utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Hive support is required to CREATE Hive TABLE (AS SELECT);\n'CreateTable `learn_spark_db`.`managed_us_delay_flights_tbl`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists\n"
     ]
    }
   ],
   "source": [
    "# Using SQL\n",
    "spark.sql(\"\"\"CREATE TABLE managed_us_delay_flights_tbl (date STRING, delay INT,\n",
    "             distance INT, origin STRING, destination STRING)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Dataframe API\n",
    "# Path to our US flight delays CSV file\n",
    "csv_file = \"../data/DelayedFlights.csv\"\n",
    "\n",
    "# Schema as defined in the preceding example\n",
    "flights_df = spark.read.option(\"inferSchema\", \"true\").csv(csv_file)\n",
    "flights_df.write.saveAsTable(\"managed_us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating an unmanaged table\n",
    "By contrast, you can create unmanaged tables from your own data sources—say, Parquet, CSV, or JSON files stored in a file store accessible to your Spark application.\n",
    "\n",
    "To create an unmanaged table from a data source such as a CSV file, in SQL use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE us_delay_flights_tbl(date STRING, delay INT,\n",
    "                distance INT, origin STRING, destination STRING)\n",
    "                USING csv OPTIONS (PATH\n",
    "                '/databricks-datasets/learning-spark-v2/flights/departuredelays.csv')\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Dataframe API\n",
    "(flights_df\n",
    "    .write\n",
    "    .option(\"path\", \"/tmp/data/us_flights_delay\")\n",
    "    .saveAsTable(\"us_delay_flights_tbl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Views\n",
    "In addition to creating tables, Spark can create views on top of existing tables. Views can be global (visible across all SparkSessions on a given cluster) or session-scoped\n",
    "(visible only to a single SparkSession), and they are temporary: they disappear after your Spark application terminates.\n",
    "\n",
    "Creating views has a similar syntax to creating tables within a database. Once you create a view, you can query it as you would a table. The difference between a view and a\n",
    "table is that views don’t actually hold the data; tables persist after your Spark application terminates, but views disappear.\n",
    "\n",
    "You can create a view from an existing table using SQL. For example, if you wish to work on only the subset of the US flight delays data set with origin airports of New\n",
    "York (JFK) and San Francisco (SFO), the following queries will create global temporary and temporary views consisting of just that slice of the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- In SQL\n",
    "CREATE OR REPLACE GLOBAL TEMP VIEW us_origin_airport_SFO_global_tmp_view AS\n",
    "    SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE\n",
    "    origin = 'SFO';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "df_sfo = spark.sql(\"\"\"SELECT date, delay, origin, destination FROM\n",
    "                      us_delay_flights_tbl WHERE origin = 'SFO'\"\"\")\n",
    "\n",
    "# Create a temporary and global temporary view\n",
    "df_sfo.createOrReplaceGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "# df_sfo.createOrReplaceTempView(\"us_origin_airport_SFO_global_tmp_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you’ve created these views, you can issue queries against them just as you would against a table. Keep in mind that when accessing a global temporary view you must use the prefix `global_temp.<view_name>`, because Spark creates global temporary views in a global temporary database called `global_temp.` For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- (Global view)\n",
    "SELECT * FROM global_temp.us_origin_airport_SFO_global_tmp_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM us_origin_airport_SFO_global_tmp_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.table(\"us_origin_airport_JFK_tmp_view\")\n",
    "# Or\n",
    "spark.sql(\"SELECT * FROM us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporary views versus global temporary views\n",
    "\n",
    "The difference between temporary and global temporary views being subtle, it can be a source of mild confusion among developers new to Spark. A temporary view is tied to a single `SparkSession` within a Spark application. In contrast, a global temporary view is visible across multiple SparkSessions within a Spark application. Yes, you can create multiple SparkSessions within a single Spark application—this can be handy, for example, in cases where you want to access (and combine) data from two different SparkSessions that don’t share the same Hive metastore configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the Metadata\n",
    "Temporary views versus global temporary views The difference between temporary and global temporary views being subtle, it can be a source of mild confusion among developers new to Spark. A temporary view is tied to a single SparkSession within a Spark application. In contrast, a global temporary view is visible across multiple SparkSessions within a Spark application. Yes, you can create multiple SparkSessions within a single Spark application—this can be handy, for example, in cases where you want to access (and combine) data from two different SparkSessions that don’t share the same Hive metastore configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='default database', locationUri='file:/C:/Users/dm/Documents/Github/training-bigdata-lab/Section%2005%20Apache%20Spark%20(PySpark)/spark-warehouse'),\n",
       " Database(name='learn_spark_db', description='', locationUri='file:/C:/Users/dm/Documents/Github/training-bigdata-lab/Section%2005%20Apache%20Spark%20(PySpark)/spark-warehouse/learn_spark_db.db')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='us_delay_flights_tbl', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.catalog.listColumns(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching SQL Tables\n",
    "We will discuss table caching stragies further later but for now, like DataFrames, you can cache and uncache SQL tables and views.\n",
    "In Spark 3.0, in addition to other options, you can specify a table as LAZY, meaning that it should only be cached when it is first used instead of immediately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CACHE [LAZY] TABLE <table-name>\n",
    "--\n",
    "UNCACHE TABLE <table-name>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Tables into DataFrames\n",
    "Often, data engineers build data pipelines as part of their regular data ingestion and ETL processes. They populate Spark SQL databases and tables with cleansed data for consumption by applications downstream.\n",
    "Let’s assume you have an existing database, `learn_spark_db`, and table, `us_delay_flights_tbl`, ready for use. Instead of reading from an external JSON file, you can simply use SQL to query the table and assign the returned result to a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# us_flights_df = spark.sql(\"SELECT * FROM us_delay_flights_tbl\")\n",
    "us_flights_df2 = spark.table(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources for DataFrames and SQL Tables\n",
    "Spark SQL provides an interface to a variety of data sources. It also provides a set of common methods for reading and writing data to and from these data sources using the <a href=\"https://www.databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html\">Data Sources API</a>.\n",
    "\n",
    "### DataFrameReader\n",
    "DataFrameReader is the core construct for reading data from a data source into a DataFrame. It has a defined format and a recommended pattern for usage:\n",
    "\n",
    "`DataFrameReader.format(args).option(\"key\", \"value\").schema(args).load()`\n",
    "\n",
    "Note that you can only access a DataFrameReader through a SparkSession instance. That is, you cannot create an instance of DataFrameReader. To get an instance handle to it, use:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkSession.read\n",
    "# or\n",
    "SparkSession.readStream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While `read` returns a handle to `DataFrameReader` to read into a DataFrame from a static data source, `readStream` returns an instance to read from a streaming source.\n",
    "\n",
    "| Method      | Arguments | Description     |\n",
    "| :----:      |    :----   |    :----:     |\n",
    "| format()      | \"parquet\", \"csv\", \"txt\", \"json\", \"jdbc\", \"orc\", \"avro\", etc.       | If you don’t specify this method, then the default is Parquet or whatever is set in spark.sql.sources.default.   |\n",
    "| option()   | (\"mode\", {PERMISSIVE , FAILFAST, DROPMALFORMED } ) (\"inferSchema\", {true , false}) (\"path\", \"path_file_data_source\")        | A series of key/value pairs and options. The Spark documentation shows some examples and explains the different modes and their actions. The default mode is PERMISSIVE. The \"inferSchema\" and \"mode\" options are specific to the JSON and CSV file formats.     |\n",
    "| schema()   | DDL String or StructType, e.g., 'A INT, B STRING' or StructType(...)        | For JSON or CSV format, you can specify to infer the schema in the option() method. Generally, providing a schema for any format makes loading faster and ensures your data conforms to the  expected schema.      |\n",
    "| load()   | \"/path/to/data/source\"        | The path to the data source. This can be empty if specified in option(\"path\", \"...\").     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet\n",
    "df = spark.read.format(\"parquet\").load(\"path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV\n",
    "df3 = spark.read.format(\"csv\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").option(\"mode\", \"PERMISSIVE\").load(\"/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON\n",
    "df4 = spark.read.format(\"json\").load(\"/databricks-datasets/learning-spark-v2/flights/summary-data/json/*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note!</b> Parquet is the default and preferred data source for Spark because it’s efficient, uses columnar storage, and employs a fast compression algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrameWriter\n",
    "\n",
    "`DataFrameWriter` does the reverse of its counterpart: it saves or writes data to a specified built-in data source. Unlike with DataFrameReader, you access its instance not from a SparkSession but from the DataFrame you wish to save. It has a few recommended usage patterns:\n",
    "\n",
    "`DataFrameWriter.format(args).option(args).sortBy(args).saveAsTable(table)`\n",
    "\n",
    "| Method      | Arguments | Description     |\n",
    "| :----:      |    :----   |    :----:     |\n",
    "| format()      | \"parquet\", \"csv\", \"txt\", \"json\", \"jdbc\", \"orc\", \"avro\", etc.       | If you don’t specify this method, then the default is Parquet or whatever is set in spark.sql.sources.default.   |\n",
    "| option()   | (\"mode\", {append , overwrite, ignore , error or errorifexists} )(\"mode\", {SaveMode.Overwrite, SaveMode.Append, SaveMode.Ignore, SaveMode.ErrorIfExists}) (\"path\", \"path_to_write_to\")        | A series of key/value pairs and options. The Spark documentation shows some examples. This is an overloaded method. The default mode options are error or error ifexists and SaveMode.ErrorIfExists; they throw an exception at runtime if the data already exists.     |\n",
    "| bucketBy()   | (numBuckets, col, col...,col)        | The number of buckets and names of columns to bucket by. Uses Hive’s bucketing scheme on a filesystem.      |\n",
    "| save()   | \"/path/to/data/source\"        | The path to save to. This can be empty if specified in option(\"path\", \"...\").     |\n",
    "| saveAsTable()  | \"table_name\"            | The table to save to.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note!</b> You can find PARQUET options for DataFrameReader and DataFrameWriter in this <a href=\"https://spark.apache.org/docs/latest/sql-data-sources-parquet.html\">link</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet\n",
    "Parquet, because it’s the default data source in Spark. Supported and widely used by many big data processing frameworks and platforms, Parquet is an open source columnar file format that offers many I/O optimizations (such as compression, which saves storage space and allows for quick access to data columns).\n",
    "Because of its efficiency and these optimizations, we recommend that after you have transformed and cleansed your data, you save your DataFrames in the Parquet format for downstream consumption.\n",
    "\n",
    "#### Reading Parquet files into a DataFrame\n",
    "Parquet files are stored in a directory structure that contains the data files, metadata, a number of compressed files, and some status files. Metadata in the footer contains\n",
    "the version of the file format, the schema, and column data such as the path, etc. For example, a directory in a Parquet file might contain a set of files like this:\n",
    "\n",
    "            _SUCCESS\n",
    "            _committed_1799640464332036264\n",
    "            _started_1799640464332036264\n",
    "            part-00000-tid-1799640464332036264-91273258-d7ef-4dc7-<...>-c000.snappy.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Parquet files into a Spark SQL table\n",
    "As well as reading Parquet files into a Spark DataFrame, you can also create a Spark SQL unmanaged table or view directly using SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- In SQL\n",
    "CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n",
    "    USING parquet\n",
    "    OPTIONS (\n",
    "    path \"<path>\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you’ve created the table or view, you can read data into a DataFrame using SQL, as we saw in some earlier examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing DataFrames to Parquet files\n",
    "Writing or saving a DataFrame as a table or file is a common operation in Spark. To write a DataFrame you simply use the methods and arguments to the DataFrame\n",
    "Writer outlined earlier in this chapter, supplying the location to save the Parquet file to. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.write.format(\"parquet\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"compression\", \"snappy\")\n",
    "    .save(\"/tmp/data/parquet/df_parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing DataFrames to Spark SQL tables\n",
    "\n",
    "Writing a DataFrame to a SQL table is as easy as writing to a file just use `saveAsTable()` instead of `save()`. This will create a managed table called `us_delay_flights_tbl`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.write\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"us_delay_flights_tbl\"))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON\n",
    "JavaScript Object Notation (JSON) is also a popular data format. It came to prominence as an easy-to-read and easy-to-parse format compared to XML. It has two representational\n",
    "formats: <a href=\"https://docs.databricks.com/external-data/json.html\">single-line mode and multiline mode</a>. Both modes are supported in Spark.\n",
    "\n",
    "#### Reading a JSON file into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"/databricks-datasets/learning-spark-v2/flights/summary-data/json/*\"\n",
    "df = spark.read.format(\"json\").load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading a JSON file into a Spark SQL table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n",
    "    USING json\n",
    "    OPTIONS (\n",
    "    path \"<path>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing DataFrames to JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.write.format(\"json\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"compression\", \"snappy\")\n",
    "    .save(\"/path/to/folder\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note!</b> You can find JSON options for DataFrameReader and DataFrameWriter in this <a href=\"https://spark.apache.org/docs/latest/sql-data-sources-json.html\">link</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV\n",
    "As widely used as plain text files, this common text file format captures each field delimited by a comma; each line with comma-separated fields represents a record. Even though a comma is the default separator, you may use other delimiters to separate fields in cases where commas are part of your data. Popular spreadsheets can generate CSV files, so it’s a popular format among data and business analysts.\n",
    "#### Reading a CSV file into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*\"\n",
    "schema = \"DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count INT\"\n",
    "\n",
    "df = (spark.read.format(\"csv\")\n",
    "            .option(\"header\", \"true\")\n",
    "            .schema(schema)\n",
    "            .option(\"mode\", \"FAILFAST\") # Exit if any errors\n",
    "            .option(\"nullValue\", \"\") # Replace any null data field with quotes\n",
    "            .load(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading a CSV file into a Spark SQL table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n",
    "    USING csv\n",
    "    OPTIONS (\n",
    "    path \"/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*\",\n",
    "    header \"true\",\n",
    "    inferSchema \"true\",\n",
    "    mode \"FAILFAST\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note!</b> You can find CSV options for DataFrameReader and DataFrameWriter in this <a href=\"https://spark.apache.org/docs/latest/sql-data-sources-csv.html\">link</a>.\n",
    "\n",
    "<b>Note!</b> For the full list of possible data sources check this <a href=\"https://spark.apache.org/docs/latest/sql-data-sources.html\">link</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('spark38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9cc7abf6a71b1e36bffb4894f1eb166079ff0aa51aabb43b5623fbc056acdf8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
