{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>The DataFrame API</h1>\n",
    "\n",
    "Inspired by pandas DataFrames in structure, format, and a few specific operations, Spark DataFrames are like distributed in-memory tables with named columns and schemas, where each column has a specific data type: integer, string, array, map, real, date, timestamp, etc.\n",
    "\n",
    "When data is visualized as a structured table, it’s not only easy to digest but also easy to work with when it comes to common operations you might want to execute on rows and columns. DataFrames are immutable and Spark keeps a lineage of all transformations. You can add or change\n",
    "the names and data types of the columns, creating new DataFrames while the previous versions are preserved. A named column in a DataFrame and its associated Spark data type can be declared in the schema.\n",
    "\n",
    "Spark supports basic <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/data_types.html\">Python data types</a>, as enumerated below:\n",
    "\n",
    "| Data type      | Value in Python | API to instantiate     |\n",
    "| :----:      |    :----:   |    :----:     |\n",
    "| ByteType      | int       | DataTypes.ByteType   |\n",
    "| ShortType   | int        | DataTypes.ShortType      |\n",
    "| IntegerType   | int        | DataTypes.IntegerType      |\n",
    "| LongType   | int        | DataTypes.LongType     |\n",
    "| FloatType   | float        | DataTypes.FloatType      |\n",
    "| DoubleType   | float        | DataTypes.DoubleType      |\n",
    "| StringType   | str        | DataTypes.StringType     |\n",
    "| BooleanType   | bool        | DataTypes.BooleanType      |\n",
    "| DecimalType   | decimal.Decimal        | DecimalType      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Spark’s Structured and Complex Data Types</h2>\n",
    "\n",
    "For complex data analytics, you won’t deal only with simple or basic data types. Your data will be complex, often structured or nested, and you’ll need Spark to handle these complex data types. They come in many forms: maps, arrays, structs, dates, timestamps, fields, etc. <a href=\"https://spark.apache.org/docs/latest/sql-ref-datatypes.html\"></a>\n",
    "\n",
    "| Data type      | Value in Python | API to instantiate     |\n",
    "| :----:      |    :----   |    :----:     |\n",
    "| BinaryType      | bytearray       | BinaryType()   |\n",
    "| TimestampType   | datetime.datetime        | TimestampType()     |\n",
    "| DateType   | datetime.date        | DateType()      |\n",
    "| ArrayType   | List, tuple, or array        | ArrayType(dataType, [nullable])     |\n",
    "| MapType   | dict        | MapType(keyType, valueType, [nullable])      |\n",
    "| StructType   | List or tuple        | StructType([fields])    |\n",
    "| StructField   | A value type corresponding to the type of this field        | StructField(name, dataType, [nullable])     |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schemas and Creating DataFrames\n",
    "\n",
    "A schema in Spark defines the column names and associated data types for a Data‐Frame. Most often, schemas come into play when you are reading structured data from an external data source. Defining a schema up front as opposed to taking a schema-on-read approach offers three benefits:\n",
    "- You relieve Spark from the complexity of inferring data types.\n",
    "- You prevent Spark from creating a separate job just to read a large portion of your file to ascertain the schema, which for a large data file can be expensive and time-consuming.\n",
    "- You can detect errors early if data doesn’t match the schema.\n",
    "\n",
    "So, I encourage you to always define your schema up front whenever you want to read a large file from a data source.\n",
    "\n",
    "### Two ways to define a schema\n",
    "\n",
    "Spark allows you to define a schema in two ways. One is to define it programmatically, and the other is to employ a Data Definition Language (DDL) string, which is much simpler and easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "# If you know spark path you can specify it as init function parameter\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "ex_schema = StructType([StructField(\"author\", StringType(), False),\n",
    "                    StructField(\"title\", StringType(), False),\n",
    "                    StructField(\"pages\", IntegerType(), False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the same schema using DDL is much simpler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n",
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- First: string (nullable = true)\n",
      " |-- Last: string (nullable = true)\n",
      " |-- Url: string (nullable = true)\n",
      " |-- Published: string (nullable = true)\n",
      " |-- Hits: integer (nullable = true)\n",
      " |-- Campaigns: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Define schema for our data using DDL\n",
    "schema = \"`Id` INT, `First` STRING, `Last` STRING, `Url` STRING, `Published` STRING, `Hits` INT, `Campaigns` ARRAY<STRING>\"\n",
    "\n",
    "# Create our static data\n",
    "data = [\n",
    "            [1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\", \"LinkedIn\"]],\n",
    "            [2, \"Brooke\",\"Wenig\", \"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\", \"LinkedIn\"]],\n",
    "            [3, \"Denny\", \"Lee\", \"https://tinyurl.3\", \"6/7/2019\", 7659, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "            [4, \"Tathagata\", \"Das\", \"https://tinyurl.4\", \"5/12/2018\", 10568, [\"twitter\", \"FB\"]],\n",
    "            [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "            [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568, [\"twitter\", \"LinkedIn\"]]\n",
    "        ]\n",
    "\n",
    "# Main program\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a SparkSession\n",
    "    spark = (SparkSession\n",
    "                    .builder\n",
    "                    .appName(\"Example-3_6\")\n",
    "                    .getOrCreate())\n",
    "\n",
    "    # Create a DataFrame using the schema defined above\n",
    "    blogs_df = spark.createDataFrame(data, schema)\n",
    "    \n",
    "    # Show the DataFrame; it should reflect our table above\n",
    "    blogs_df.show()\n",
    "    \n",
    "    # Print the schema used by Spark to process the DataFrame\n",
    "    blogs_df.printSchema()\n",
    "    \n",
    "    # spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use this schema elsewhere in your code, simply execut `blogs_df.schema` and it will return the schema definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('Id', IntegerType(), True), StructField('First', StringType(), True), StructField('Last', StringType(), True), StructField('Url', StringType(), True), StructField('Published', StringType(), True), StructField('Hits', IntegerType(), True), StructField('Campaigns', ArrayType(StringType(), True), True)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogs_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns and Expressions\n",
    "As mentioned previously, named columns in DataFrames are conceptually similar to named columns in pandas or R DataFrames or in an RDBMS table: they describe a type of field. You can list all the columns by their names, and you can perform operations on their values using relational or computational expressions. In Spark’s supported languages, columns are objects with public methods.\n",
    "\n",
    "Let’s take a look at some examples of what we can do with columns in Spark. Each example is followed by its output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Id', 'First', 'Last', 'Url', 'Published', 'Hits', 'Campaigns']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "blogs_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Id', 'First', 'Last', 'Url', 'Published', 'Hits', 'Campaigns']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access a particular column with col and it returns a Column type\n",
    "blogs_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|(Hits * 2)|\n",
      "+----------+\n",
      "|      9070|\n",
      "|     17816|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use an expression to compute a value\n",
    "# blogsDF.select(expr(\"Hits * 2\")).show(2)\n",
    "# or use col to compute value\n",
    "# Example of tow columns (expr(\"columnName - 5\") > col(anothercolumnName)),\n",
    "blogs_df.select(col(\"Hits\") * 2).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|Big Hitters|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|      false|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|      false|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|      false|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|       true|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|       true|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|       true|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use an expression to compute big hitters for blogs\n",
    "# This adds a new column, Big Hitters, based on the conditional expression\n",
    "blogs_df.withColumn(\"Big Hitters\", (expr(\"Hits > 10000\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|    AuthorsId|\n",
      "+-------------+\n",
      "|  JulesDamji1|\n",
      "| BrookeWenig2|\n",
      "|    DennyLee3|\n",
      "|TathagataDas4|\n",
      "+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs_df.withColumn(\"AuthorsId\", (concat(expr(\"First\"), expr(\"Last\"), expr(\"Id\")))) \\\n",
    "    .select(col(\"AuthorsId\")) \\\n",
    "    .show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The Below code is identical\n",
    "# blogs_df.select(expr(\"Hits\")).show(2)\n",
    "# blogs_df.select(col(\"Hits\")).show(2)\n",
    "blogs_df.select(\"Hits\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('Id', IntegerType(), True), StructField('First', StringType(), True), StructField('Last', StringType(), True), StructField('Url', StringType(), True), StructField('Published', StringType(), True), StructField('Hits', IntegerType(), True), StructField('Campaigns', ArrayType(StringType(), True), True)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogs_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort by column \"Id\" in descending order\n",
    "# blogs_df.sort(col(\"Id\"), ascending=False).show()\n",
    "blogs_df.sort(\"Id\", ascending=False).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rows\n",
    "A row in Spark is a generic Row object, containing one or more columns. Each column may be of the same data type (e.g., integer or string), or they can have different types (integer, string, map, array, etc.). Because Row is an object in Spark and an ordered collection of fields, you can instantiate a Row in each of Spark’s supported languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|      Authors|State|\n",
      "+-------------+-----+\n",
      "|Matei Zaharia|   CA|\n",
      "|  Reynold Xin|   CA|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows = [Row(\"Matei Zaharia\", \"CA\"), Row(\"Reynold Xin\", \"CA\")]\n",
    "\n",
    "authors_df = spark.createDataFrame(rows, [\"Authors\", \"State\"])\n",
    "\n",
    "authors_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common DataFrame Operations\n",
    "To perform common data operations on DataFrames, you’ll first need to load a Data‐Frame from a data source that holds your structured data. Spark provides an interface, `DataFrameReader`, that enables you to read data into a DataFrame from myriad data sources in formats such as JSON, CSV, Parquet, Text, Avro, ORC, etc. Likewise, to write a DataFrame back to a data source in a particular format, Spark uses `DataFrameWriter`.\n",
    "\n",
    "### Using DataFrameReader and DataFrameWriter\n",
    "\n",
    "Reading and writing are simple in Spark because of these high-level abstractions and contributions from the community to connect to a wide variety of data sources, including common NoSQL stores, RDBMSs, streaming engines such as Apache Kafka and Kinesis, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------+--------+-------+--------+-------+\n",
      "|      Date|   Open|    High|     Low|  Close|AdjClose| Volume|\n",
      "+----------+-------+--------+--------+-------+--------+-------+\n",
      "|2020-05-20|1389.58| 1410.42| 1387.25|1406.72| 1406.72|1655400|\n",
      "|2020-05-21| 1408.0| 1415.49| 1393.45| 1402.8|  1402.8|1385000|\n",
      "|2020-05-22|1396.71| 1412.76| 1391.83|1410.42| 1410.42|1309400|\n",
      "|2020-05-26|1437.27|  1441.0| 1412.13|1417.02| 1417.02|2060600|\n",
      "|2020-05-27|1417.25| 1421.74| 1391.29|1417.84| 1417.84|1685800|\n",
      "|2020-05-28|1396.86| 1440.84|  1396.0|1416.73| 1416.73|1692200|\n",
      "|2020-05-29|1416.94| 1432.57| 1413.35|1428.92| 1428.92|1838100|\n",
      "|2020-06-01|1418.39| 1437.96|  1418.0|1431.82| 1431.82|1217100|\n",
      "|2020-06-02|1430.55| 1439.61| 1418.83|1439.22| 1439.22|1278100|\n",
      "|2020-06-03| 1438.3|1446.552|1429.777|1436.38| 1436.38|1256200|\n",
      "|2020-06-04| 1430.4| 1438.96| 1404.73|1412.18| 1412.18|1484300|\n",
      "|2020-06-05|1413.17| 1445.05|  1406.0|1438.39| 1438.39|1734900|\n",
      "|2020-06-08|1422.34| 1447.99| 1422.34|1446.61| 1446.61|1404200|\n",
      "|2020-06-09|1445.36|  1468.0| 1443.21|1456.16| 1456.16|1409200|\n",
      "|2020-06-10|1459.54|1474.259| 1456.27|1465.85| 1465.85|1525200|\n",
      "|2020-06-11|1442.48|1454.475|  1402.0|1403.84| 1403.84|1991300|\n",
      "|2020-06-12|1428.49|  1437.0| 1386.02|1413.18| 1413.18|1944200|\n",
      "|2020-06-15| 1390.8|  1424.8| 1387.92|1419.85| 1419.85|1503900|\n",
      "|2020-06-16|1445.22| 1455.02|  1425.9|1442.72| 1442.72|1709200|\n",
      "|2020-06-17|1447.16|  1460.0| 1431.38|1451.12| 1451.12|1548300|\n",
      "+----------+-------+--------+--------+-------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In Python, define a schema\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Programmatic way to define a schema\n",
    "fire_schema = StructType([StructField('Date', DateType(), True),\n",
    "                            StructField('Open', FloatType(), True),\n",
    "                            StructField('High', FloatType(), True),\n",
    "                            StructField('Low', FloatType(), True),\n",
    "                            StructField('Close', FloatType(), True),\n",
    "                            StructField('AdjClose', FloatType(), True),\n",
    "                            StructField('Volume', IntegerType(), True)])\n",
    "\n",
    "# Use the DataFrameReader interface to read a CSV file\n",
    "sf_fire_file = \"../data/GOOG.csv\"\n",
    "goog_df = spark.read.csv(sf_fire_file, header=True, schema=fire_schema)\n",
    "\n",
    "goog_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To write the DataFrame into an external data source in your format of choice, you can use the DataFrameWriter interface. Like DataFrameReader, it supports multiple data sources. `Parquet, a popular columnar format, is the default format; it uses snappy compression to compress the data. If the DataFrame is written as Parquet, the schema is preserved as part of the Parquet metadata. In this case, subsequent reads back into a DataFrame do not require you to manually supply a schema`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save as a Parquet file\n",
    "parquet_path = \"../data/goog.parquet\"\n",
    "goog_df.write.format(\"parquet\").save(parquet_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can save it as a table, which registers metadata with the Hive metastore(we will cover SQL managed and unmanaged tables, metastores, and Data‐Frames in the next chapter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_table = \"demoTable\" # name of the table\n",
    "goog_df.write.format(\"parquet\").saveAsTable(parquet_table)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations and actions\n",
    "Now that you have a distributed DataFrame composed of GOOG data in memory, the first thing you as a developer will want to do is examine\n",
    "your data to see what the columns look like. Are they of the correct types? Do any of them need to be converted to different types? Do they have `null` values?\n",
    "\n",
    "### Projections and filters\n",
    "A projection in relational parlance is a way to return only the rows matching a certain relational condition by using filters. In Spark, projections are done with the select() method, while filters can be expressed using the filter() or where() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------+\n",
      "|Date      |AdjClose|Volume |\n",
      "+----------+--------+-------+\n",
      "|2020-05-20|1406.72 |1655400|\n",
      "|2020-05-21|1402.8  |1385000|\n",
      "|2020-05-26|1417.02 |2060600|\n",
      "|2020-05-27|1417.84 |1685800|\n",
      "|2020-05-28|1416.73 |1692200|\n",
      "+----------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "few_goog_df = (goog_df.select(\"Date\", \"AdjClose\", \"Volume\")\n",
    "                    .where(col(\"Volume\") >= 1384000))\n",
    "\n",
    "few_goog_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to know how many distinct `Dates` are there? These simple and expressive queries do the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|DistinctDates|\n",
      "+-------------+\n",
      "|           51|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(few_goog_df.select(\"Date\")\n",
    "            .where(col(\"Date\").isNotNull())\n",
    "            .agg(countDistinct(\"Date\").alias(\"DistinctDates\"))\n",
    "            .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can list the distinct call types in the data set using these queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|Date      |\n",
      "+----------+\n",
      "|2020-07-24|\n",
      "|2020-08-05|\n",
      "|2020-06-04|\n",
      "|2020-06-05|\n",
      "|2020-08-04|\n",
      "|2020-06-17|\n",
      "|2020-07-02|\n",
      "|2020-06-12|\n",
      "|2020-08-07|\n",
      "|2020-08-13|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter for only distinct non-null CallTypes from all the rows\n",
    "(few_goog_df.select(\"Date\")\n",
    "            .where(col(\"Date\").isNotNull())\n",
    "            .distinct()\n",
    "            .show(10, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming, adding, and dropping columns\n",
    "Sometimes you want to rename particular columns for reasons of style or convention, and at other times for readability or brevity. The original column names in the data set may had spaces in them. Spaces in column names can be problematic, especially when you want to write or save a\n",
    "DataFrame as a Parquet file (which prohibits this).\n",
    "\n",
    "By specifying the desired column names in the schema with StructField, as we did, we effectively changed all names in the resulting DataFrame. Alternatively, you could selectively rename columns with the `withColumnRenamed()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|NewVolume|\n",
      "+---------+\n",
      "|1655400  |\n",
      "|1385000  |\n",
      "|2060600  |\n",
      "|1685800  |\n",
      "|1692200  |\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_goog_df = goog_df.withColumnRenamed(\"Volume\", \"NewVolume\")\n",
    "\n",
    "(new_goog_df.select(\"NewVolume\")\n",
    "            .where(col(\"NewVolume\") > 1384000)\n",
    "            .show(5, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note!</b>  Because DataFrame transformations are immutable, when we rename a column using `withColumnRenamed()` we get a new Data-Frame while retaining the original with the old column name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifying the contents of a column or its type are common operations during data exploration. In some cases the data is raw or dirty, or its types are not amenable to being supplied as arguments to relational operators. For example, in some data sets, some columns which containe date values can be strings rather than either Unix timestamps or SQL dates, both of which Spark supports and can easily manipulate during transformations or actions (e.g., during a date- or time- based analysis of the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "# fire_ts_df = (new_fire_df\n",
    "#                 .withColumn(\"IncidentDate\", to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\"))\n",
    "#                 .drop(\"CallDate\")\n",
    "#                 .withColumn(\"OnWatchDate\", to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\"))\n",
    "#                 .drop(\"WatchDate\")\n",
    "#                 .withColumn(\"AvailableDtTS\", to_timestamp(col(\"AvailableDtTm\"), \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "#                 .drop(\"AvailableDtTm\"))\n",
    "\n",
    "# Select the converted columns\n",
    "# (fire_ts_df.select(\"IncidentDate\", \"OnWatchDate\", \"AvailableDtTS\")\n",
    "#     .show(5, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previos codes:\n",
    "1. Convert the existing column’s data type from string to a Spark-supported timestamp.\n",
    "2. Use the new format specified in the format string \"MM/dd/yyyy\" or \"MM/dd/yyyy hh:mm:ss a\" where appropriate.\n",
    "3. After converting to the new data type, drop() the old column and append the new one specified in the first argument to the withColumn() method.\n",
    "4. Assign the new modified DataFrame to fire_ts_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov  4 2022, 15:16:59) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9cc7abf6a71b1e36bffb4894f1eb166079ff0aa51aabb43b5623fbc056acdf8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
