{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with External Data Sources\n",
    "In this notebook, we will focus on how Spark SQL interfaces with external components. Specifically, we discuss how Spark SQL allows you to:\n",
    "- Use user-defined functions for both Apache Hive and Apache Spark.\n",
    "- Connect with external data sources such as JDBC and SQL databases, PostgreSQL, MySQL, Tableau, Azure Cosmos DB, and MS SQL Server.\n",
    "- Work with simple and complex types, higher-order functions, and common relationa operators.\n",
    "\n",
    "## Spark SQL and Apache Hive\n",
    "Spark SQL lets Spark programmers leverage the benefits of faster performance and relational programming (e.g., declarative queries and optimized storage), as well as call complex analytics libraries (e.g., machine learning).\n",
    "\n",
    "### User-Defined Functions\n",
    "Apache Spark provides the flexibility that allows for data engineers and data scientists to define their own functions too. These are known as `user-defined functions (UDFs)`.\n",
    "\n",
    "#### Spark SQL UDFs\n",
    "The benefit of creating your own PySpark UDFs is that you (and others) will be able to make use of them within Spark SQL itself. For example, a data scientist can wrap an ML model within a UDF so that a data analyst can query its predictions in Spark SQL without necessarily understanding the internals of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "# If you know spark path you can specify it as init function parameter\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = (SparkSession\n",
    "            .builder\n",
    "            .appName(\"SparkSQLandDFsPart2\")\n",
    "            .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# Create cubed function\n",
    "def cubed(s):\n",
    "    return s * s * s\n",
    "\n",
    "# Register UDF\n",
    "spark.udf.register(\"cubed\", cubed, LongType())\n",
    "\n",
    "# Generate temporary view\n",
    "spark.range(1, 9).createOrReplaceTempView(\"udf_test\")\n",
    "\n",
    "# Query the cubed UDF\n",
    "spark.sql(\"SELECT id, cubed(id) AS id_cubed FROM udf_test\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation order and null checking in Spark SQL\n",
    "Spark SQL (this includes SQL, the DataFrame API, and the Dataset API) does not guarantee the order of evaluation of subexpressions. For example, the following query does not guarantee that the s is NOT NULL clause is executed prior to the strlen(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT s FROM test1 WHERE s IS NOT NULL AND strlen(s) > 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, to perform proper `null` checking, it is recommended that you do the following:\n",
    "1. Make the UDF itself null-aware and do n`ull checking inside the UDF.\n",
    "2. Use `IF` or `CASE WHEN` expressions to do the `null` check and invoke the UDF in a conditional branch.\n",
    "\n",
    "#### Speeding up and distributing PySpark UDFs with Pandas UDFs\n",
    "One of the previous prevailing issues with using PySpark UDFs was that they had slower performance than Scala UDFs. This was because the PySpark UDFs required data movement between the JVM and Python, which was quite expensive. To resolve this problem, Pandas UDFs (also known as vectorized UDFs). A Pandas UDF uses Apache Arrow to transfer data and Pandas to work with the data. You define a Pandas UDF using the keyword pandas_udf as the decorator. Instead of operating on individual inputs row by row, you are operating on a Pandas Series or DataFrame (i.e., vectorized execution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Import various pyspark SQL functions including pandas_udf\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# Declare the cubed function\n",
    "def cubed(a: pd.Series) -> pd.Series:\n",
    "    return a * a * a\n",
    "\n",
    "# Create the pandas UDF for the cubed function\n",
    "cubed_udf = pandas_udf(cubed, returnType=LongType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s start with a simple Pandas Series (as defined for x) and then apply the local function `cubed()` for the cubed calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pandas Series\n",
    "x = pd.Series([1, 2, 3])\n",
    "\n",
    "# The function for a pandas_udf executed with local Pandas data\n",
    "print(cubed(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s switch to a Spark DataFrame. We can execute this function as a Spark vectorized UDF as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark DataFrame, 'spark' is an existing SparkSession\n",
    "df = spark.range(1, 4)\n",
    "\n",
    "# Execute function as a Spark vectorized UDF\n",
    "df.select(\"id\", cubed_udf(col(\"id\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying with the Spark SQL Shell, Beeline\n",
    "There are various mechanisms to query Apache Spark, including the Spark SQL shell, the Beeline CLI utility, and reporting tools like Tableau and <a href=\"https://learn.microsoft.com/en-gb/azure/databricks/partners/bi/power-bi\">Power BI</a>.\n",
    "### Using the Spark SQL Shell\n",
    "A convenient tool for executing Spark SQL queries is the spark-sql CLI. While this utility communicates with the Hive metastore service in local mode, it does not talk to the `Thrift JDBC/ODBC server` (a.k.a. Spark Thrift Server or STS). `The STS allows JDBC/ODBC clients to execute SQL queries over JDBC and ODBC protocols on Apache Spark`.\n",
    "To start the Spark SQL CLI, execute the following command in the `$SPARK_HOME` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "./bin/spark-sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE TABLE people (name STRING, age int);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert data into the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Insert from another existing table\n",
    "INSERT INTO people SELECT name, age FROM ...\n",
    "\n",
    "-- Insert values directly\n",
    "INSERT INTO people VALUES (\"Michael\", NULL);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running a Spark SQL query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SHOW TABLES;\n",
    "\n",
    "SELECT * FROM people WHERE age < 20;\n",
    "\n",
    "SELECT name FROM people WHERE age IS NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Beeline\n",
    "If you’ve worked with Apache Hive you may be familiar with the command-line tool Beeline, a common utility for running HiveQL queries against HiveServer2. Beeline is a JDBC client based on the SQLLine CLI. You can use this same utility to execute Spark SQL queries against the Spark Thrift server\n",
    "\n",
    "<b>Note!</b> Spark thrift server is pretty similar to hiveserver2 thrift, rather submitting the sql queries as hive mr job it will use spark SQL engine which underline uses full spark capabilities. As an use case tools like Tableau can easily connect to spark thrift server through ODBC driver just like hiveserver2 and access the hive or spark temp tables to run the sql queries on spark framework.\n",
    "\n",
    "#### Connect to the Thrift server via Beeline\n",
    "To test the Thrift JDBC/ODBC server using Beeline, execute the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "./bin/beeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!connect jdbc:hive2://<hostname>:<port>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SHOW tables;\n",
    "\n",
    "SELECT * FROM people;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External Data Sources\n",
    "\n",
    "### JDBC and SQL Databases\n",
    "Spark SQL includes a data source API that can read data from other databases using JDBC. It simplifies querying these data sources as it returns the results as a Data‐Frame.\n",
    "To get started, you will need to specify the JDBC driver for your JDBC data source and it will need to be on the Spark classpath. From the `$SPARK_HOME` folder, you’ll issue a command like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note!</b> JDBC common connection properties in this <a href=\"https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html\">link</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The importance of partitioning\n",
    "When transferring large amounts of data between Spark SQL and a JDBC external source, it is important to partition your data source. All of your data is going through one driver connection, which can saturate and significantly slow down the performance of your extraction, as well as  potentially saturate the resources of your source system. While these JDBC properties are optional, for any large-scale operations it is highly recommended to use the properties shown below:\n",
    "| Property name      | Description |\n",
    "| :----:      |    :----   |\n",
    "| numPartitions      | The maximum number of partitions that can be used for parallelism in table reading and writing. This also determines the maximum number of concurrent JDBC connections.       |\n",
    "| partitionColumn   | When reading an external source, `partitionColumn` is the column that is used to determine the partitions; note, `partitionColumn` must be a numeric, date, or timestamp column.        |\n",
    "| lowerBound   | Sets the minimum value of `partitionColumn` for the partition stride.        |\n",
    "| upperBound   | Sets the maximum value of `partitionColumn` for the partition stride.        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "- numPartitions: 10\n",
    "- lowerBound: 1000\n",
    "- upperBound: 10000\n",
    "\n",
    "Then the stride is equal to 1,000, and 10 partitions will be created. This is the equivalent of executing these 10 queries (one for each partition):\n",
    "- SELECT * FROM table WHERE partitionColumn BETWEEN 1000 and 2000\n",
    "- SELECT * FROM table WHERE partitionColumn BETWEEN 2000 and 3000\n",
    "- ...\n",
    "- SELECT * FROM table WHERE partitionColumn BETWEEN 9000 and 10000\n",
    "\n",
    "While not all-encompassing, the following are some hints to keep in mind when using these properties:\n",
    "- A good starting point for numPartitions is to use a multiple of the number of Spark workers. For example, if you have four Spark worker nodes, then perhaps start with 4 or 8 partitions. But it is also important to note how well your source system can handle the read requests.\n",
    "- Initially, calculate the lowerBound and upperBound based on the minimum and maximum partitionColumn actual values. For example, if you choose\n",
    "{numPartitions:10, lowerBound: 1000, upperBound: 10000}, but all of the values are between 2000 and 5000, then only 3 of the 10 queries (one for each partition) will be doing all of the work. In this scenario, a better configuration would be {numPartitions:10, lowerBound: 2000, upperBound: 4000}.\n",
    "- Choose a partitionColumn that can be uniformly distributed to avoid data skew. For example, if the majority of your partitionColumn has the value 2500, with {numPartitions:10, lowerBound: 1000, upperBound: 10000} most of the work will be performed by the task requesting the values between 2000 and 3000. Instead, choose a different partitionColumn, or if possible generate a new one (perhaps a hash of multiple columns) to more evenly distribute your partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PostgreSQL\n",
    "To connect to a PostgreSQL database, build or download the JDBC jar from Maven and add it to your classpath. Then start a Spark shell (spark-shell or pyspark), specifying that jar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "bin/spark-shell --jars postgresql-42.2.6.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following examples show how to load from and save to a PostgreSQL database using the Spark SQL data source API and JDBC in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Option 1: Loading data from a JDBC source using load method\n",
    "jdbcDF1 = (spark\n",
    "            .read\n",
    "            .format(\"jdbc\")\n",
    "            .option(\"url\", \"jdbc:postgresql://[DBSERVER]\")\n",
    "            .option(\"dbtable\", \"[SCHEMA].[TABLENAME]\")\n",
    "            .option(\"user\", \"[USERNAME]\")\n",
    "            .option(\"password\", \"[PASSWORD]\")\n",
    "            .load())\n",
    "\n",
    "# Read Option 2: Loading data from a JDBC source using jdbc method\n",
    "# jdbcDF2 = (spark\n",
    "#             .read\n",
    "#             .jdbc(\"jdbc:postgresql://[DBSERVER]\", \"[SCHEMA].[TABLENAME]\", properties={\"user\": \"[USERNAME]\", \"password\": \"[PASSWORD]\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Option 1: Saving data to a JDBC source using save method\n",
    "(jdbcDF1\n",
    "    .write\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"url\", \"jdbc:postgresql://[DBSERVER]\")\n",
    "    .option(\"dbtable\", \"[SCHEMA].[TABLENAME]\")\n",
    "    .option(\"user\", \"[USERNAME]\")\n",
    "    .option(\"password\", \"[PASSWORD]\")\n",
    "    .save())\n",
    "\n",
    "# Write Option 2: Saving data to a JDBC source using jdbc method\n",
    "# (jdbcDF2\n",
    "#     .write\n",
    "#     .jdbc(\"jdbc:postgresql:[DBSERVER]\", \"[SCHEMA].[TABLENAME]\",\n",
    "#     properties={\"user\": \"[USERNAME]\", \"password\": \"[PASSWORD]\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MySQL\n",
    "The following examples show how to load data from and save it to a MySQL database using the Spark SQL data source API and JDBC in Scala:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data from a JDBC source using load\n",
    "jdbcDF = (spark\n",
    "            .read\n",
    "            .format(\"jdbc\")\n",
    "            .option(\"url\", \"jdbc:mysql://[DBSERVER]:3306/[DATABASE]\")\n",
    "            .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    "            .option(\"dbtable\", \"[TABLENAME]\")\n",
    "            .option(\"user\", \"[USERNAME]\")\n",
    "            .option(\"password\", \"[PASSWORD]\")\n",
    "            .load())\n",
    "\n",
    "# Saving data to a JDBC source using save\n",
    "(jdbcDF\n",
    "    .write\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"url\", \"jdbc:mysql://[DBSERVER]:3306/[DATABASE]\")\n",
    "    .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    "    .option(\"dbtable\", \"[TABLENAME]\")\n",
    "    .option(\"user\", \"[USERNAME]\")\n",
    "    .option(\"password\", \"[PASSWORD]\")\n",
    "    .save())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built-in Functions for Complex Data Types\n",
    "Instead of using these potentially expensive techniques, you may be able to use some of the built-in functions for complex data types included as part of Apache Spark 2.4 and later. Some of the more common ones are listed in this <a href=\"https://sparkbyexamples.com/spark/spark-sql-array-functions/\">link</a>.\n",
    "\n",
    "### Higher-Order Funtions\n",
    "In addition to the previously noted built-in functions, there are higher-order functions that take anonymous lambda functions as arguments. Let’s create a sample data set so we can run some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([StructField(\"celsius\", ArrayType(IntegerType()))])\n",
    "\n",
    "t_list = [[35, 36, 32, 30, 40, 42, 38]], [[31, 32, 34, 55, 56]]\n",
    "\n",
    "t_c = spark.createDataFrame(t_list, schema)\n",
    "\n",
    "t_c.createOrReplaceTempView(\"tC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### transform()\n",
    "The transform() function produces an array by applying a function to each element of the input array (similar to a map() function):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT celsius,\n",
    "             transform(celsius, t -> ((t * 9) div 5) + 32) as fahrenheit\n",
    "             FROM tC\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter()\n",
    "The filter() function produces an array consisting of only the elements of the input array for which the Boolean function is true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter temperatures > 38C for array of temperatures\n",
    "spark.sql(\"\"\"SELECT celsius,\n",
    "             filter(celsius, t -> t > 38) as high\n",
    "             FROM tC\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### exists()\n",
    "The exists() function returns true if the Boolean function holds for any element in the input array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is there a temperature of 38C in the array of temperatures\n",
    "spark.sql(\"\"\"SELECT celsius,\n",
    "             exists(celsius, t -> t = 38) as threshold\n",
    "             FROM tC\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reduce()\n",
    "The reduce() function reduces the elements of the array to a single value by merging the elements into a buffer B using function<B, T, B> and applying a finishing function<B, R> on the final buffer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average temperature and convert to F\n",
    "spark.sql(\"\"\"SELECT celsius,\n",
    "            reduce(celsius,\n",
    "            0,\n",
    "            (t, acc) -> t + acc,\n",
    "            acc -> (acc div size(celsius) * 9 div 5) + 32\n",
    "            ) as avgFahrenheit\n",
    "            FROM tC\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common DataFrames and Spark SQL Operations\n",
    "Part of the power of Spark SQL comes from the wide range of DataFrame operations (also known as untyped Dataset operations) it supports. The list of operations is quite extensive and includes:\n",
    "- Aggregate functions\n",
    "- Collection functions\n",
    "- Datetime functions\n",
    "- Math functions\n",
    "- Miscellaneous functions\n",
    "- Non-aggregate functions\n",
    "- Sorting functions\n",
    "- String functions\n",
    "For the full lit check the following <a href=\"https://spark.apache.org/docs/latest/api/sql/index.html\">link</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43998"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_location = \"../data/movie_data_part1.csv\"\n",
    "\n",
    "df_mani = spark.read.format(\"csv\") \\\n",
    "            .option(\"inferSchema\", True) \\\n",
    "            .option(\"header\", True) \\\n",
    "            .option(\"sep\", '|') \\\n",
    "            .load(file_location)\n",
    "            \n",
    "df_mani.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset Columns and View a Glimpse of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+------------------+------------+---------+--------------------+\n",
      "|   id|   budget|        popularity|release_date|  revenue|               title|\n",
      "+-----+---------+------------------+------------+---------+--------------------+\n",
      "|43000|      0.0|             2.503|  1962-05-23|      0.0|The Elusive Corporal|\n",
      "|43001|      0.0|              5.51|  1962-11-12|      0.0|  Sundays and Cybele|\n",
      "|43002|      0.0|              5.62|  1962-05-24|      0.0|Lonely Are the Brave|\n",
      "|43003|      0.0|             7.159|  1975-03-12|      0.0|          F for Fake|\n",
      "|43004| 500000.0|             3.988|  1962-10-09|      0.0|Long Day's Journe...|\n",
      "|43006|      0.0|             3.194|  1962-03-09|      0.0|           My Geisha|\n",
      "|43007|      0.0|             2.689|  1962-10-31|      0.0|Period of Adjustment|\n",
      "|43008|      0.0|             6.537|  1959-03-13|      0.0|    The Hanging Tree|\n",
      "|43010|      0.0|             4.297|  1962-01-01|      0.0|Sherlock Holmes a...|\n",
      "|43011|      0.0|             4.417|  1962-01-01|      0.0|  Sodom and Gomorrah|\n",
      "|43012|7000000.0|4.7219999999999995|  1962-11-21|4000000.0|         Taras Bulba|\n",
      "|43013|      0.0|             2.543|  1962-04-17|      0.0|The Counterfeit T...|\n",
      "|43014|      0.0|             4.303|  1962-10-24|      0.0|     Tower of London|\n",
      "|43015|      0.0|             3.493|  1962-12-07|      0.0|Varan the Unbelie...|\n",
      "|43016|      0.0|             2.851|  1962-01-01|      0.0|Waltz of the Tore...|\n",
      "|43017|      0.0|             4.047|  1961-10-11|      0.0|         Back Street|\n",
      "|43018|      0.0|             2.661|  1961-06-02|      0.0|Gidget Goes Hawaiian|\n",
      "|43019|      0.0|             3.225|  2010-05-28|      0.0|Schuks Tshabalala...|\n",
      "|43020|      0.0|              5.72|  1961-06-15|      0.0|The Colossus of R...|\n",
      "|43021|      0.0|             3.292|  2008-08-22|      0.0|          Sex Galaxy|\n",
      "+-----+---------+------------------+------------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Defining a list to subset the required columns\n",
    "select_columns=['id','budget','popularity','release_date','revenue','title']\n",
    "\n",
    "# Subsetting the required columns from the DataFrame\n",
    "df_mani = df_mani.select(*select_columns)\n",
    "\n",
    "# The following command displays the data; by default it shows top 20 rows\n",
    "df_mani.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df_mani.filter((df_mani['popularity']=='') | col('popularity').isNull() | isnan('popularity')).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to calculate all the missing values in the DataFrame, you can use thefollowing command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+------------+-------+-----+\n",
      "| id|budget|popularity|release_date|revenue|title|\n",
      "+---+------+----------+------------+-------+-----+\n",
      "|125|   125|       215|         221|    215|  304|\n",
      "+---+------+----------+------------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mani.select([count(when((col(c)=='') | col(c).isNull() | isnan(c), lit('null'))).alias(c) for c in df_mani.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Way Frequencies\n",
    "Let’s see how we can calculate the frequencies of categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|title               |count|\n",
      "+--------------------+-----+\n",
      "|null                |304  |\n",
      "|The Three Musketeers|8    |\n",
      "|Les Misérables      |8    |\n",
      "|Cinderella          |8    |\n",
      "|Hamlet              |7    |\n",
      "|Frankenstein        |7    |\n",
      "|A Christmas Carol   |7    |\n",
      "|The Island          |7    |\n",
      "|Dracula             |7    |\n",
      "|Framed              |6    |\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mani.groupBy(col('title')).count().sort(desc(\"count\")).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+----------+------------+-------+---------------------------------------+\n",
      "|id   |budget  |popularity|release_date|revenue|title                                  |\n",
      "+-----+--------+----------+------------+-------+---------------------------------------+\n",
      "|43000|0.0     |2.503     |1962-05-23  |0.0    |The Elusive Corporal                   |\n",
      "|43001|0.0     |5.51      |1962-11-12  |0.0    |Sundays and Cybele                     |\n",
      "|43002|0.0     |5.62      |1962-05-24  |0.0    |Lonely Are the Brave                   |\n",
      "|43003|0.0     |7.159     |1975-03-12  |0.0    |F for Fake                             |\n",
      "|43004|500000.0|3.988     |1962-10-09  |0.0    |Long Day's Journey Into Night          |\n",
      "|43006|0.0     |3.194     |1962-03-09  |0.0    |My Geisha                              |\n",
      "|43007|0.0     |2.689     |1962-10-31  |0.0    |Period of Adjustment                   |\n",
      "|43008|0.0     |6.537     |1959-03-13  |0.0    |The Hanging Tree                       |\n",
      "|43010|0.0     |4.297     |1962-01-01  |0.0    |Sherlock Holmes and the Deadly Necklace|\n",
      "|43011|0.0     |4.417     |1962-01-01  |0.0    |Sodom and Gomorrah                     |\n",
      "+-----+--------+----------+------------+-------+---------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp = df_mani.filter((df_mani['title']!='') & (df_mani['title'].isNotNull()) & (~isnan(df_mani['title'])))\n",
    "\n",
    "df_temp.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|title               |count|\n",
      "+--------------------+-----+\n",
      "|Cinderella          |8    |\n",
      "|The Three Musketeers|8    |\n",
      "|Les Misérables      |8    |\n",
      "|Hamlet              |7    |\n",
      "|Frankenstein        |7    |\n",
      "|The Island          |7    |\n",
      "|Dracula             |7    |\n",
      "|A Christmas Carol   |7    |\n",
      "|First Love          |6    |\n",
      "|Beauty and the Beast|6    |\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp.groupby(df_temp['title']).count().filter(\"`count` > 4\").sort(col(\"count\").desc()).show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casting Variables\n",
    "If you are not careful in identifying the datatypes, you may experience data loss on casting. For example, if you are converting a string column to a numeric one, the resulting column can be all nulls. It is good practice to explore the dataset before applying any transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'string'),\n",
       " ('budget', 'double'),\n",
       " ('popularity', 'double'),\n",
       " ('release_date', 'string'),\n",
       " ('revenue', 'double'),\n",
       " ('title', 'string')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mani.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mani = df_mani.withColumn('budget', df_temp['budget'].cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'string'),\n",
       " ('budget', 'float'),\n",
       " ('popularity', 'double'),\n",
       " ('release_date', 'string'),\n",
       " ('revenue', 'double'),\n",
       " ('title', 'string')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mani.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'int'),\n",
       " ('budget', 'float'),\n",
       " ('popularity', 'float'),\n",
       " ('release_date', 'date'),\n",
       " ('revenue', 'float'),\n",
       " ('title', 'string')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing necessary libraries\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#Identifying and assigning lists of variables\n",
    "int_vars=['id']\n",
    "float_vars=['budget', 'popularity', 'revenue']\n",
    "date_vars=['release_date']\n",
    "\n",
    "#Converting integer variables\n",
    "for column in int_vars:\n",
    "    df_mani = df_temp.withColumn(column, df_temp[column].cast(IntegerType()))\n",
    "\n",
    "for column in float_vars:\n",
    "    df_mani = df_temp.withColumn(column, df_temp[column].cast(FloatType()))\n",
    "\n",
    "for column in date_vars:\n",
    "    df_mani = df_temp.withColumn(column, df_temp[column].cast(DateType()))\n",
    "\n",
    "df_mani.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics\n",
    "To analyze any data, you should have a keen understanding of the type of data, its distribution, and its dispersion. Spark has a nice suite of built-in functions that will make it easier to quickly calculate these fields. The describe function in Spark is very handy, as it gives the count of total non-missing values for each column, mean/average, standard deviation, and minimum and maximum values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+------------------+--------------------+-------------------+--------------------+\n",
      "|summary|                  id|              budget|        popularity|        release_date|            revenue|               title|\n",
      "+-------+--------------------+--------------------+------------------+--------------------+-------------------+--------------------+\n",
      "|  count|               43873|               43873|             43783|               43777|              43783|               43694|\n",
      "|   mean|  44502.304312077475|  3736901.8351683044| 5.295444259187363|                null|  9697079.597131306|            Infinity|\n",
      "| stddev|  27189.646588626394|1.5871814953840714E7| 6.168030519876136|                null|5.687938447592969E7|                 NaN|\n",
      "|    min|\"[{'id': 104, 'lo...|                 0.0|0.6000000000000001|        1 Giant Leap|                0.0|!Women Art Revolu...|\n",
      "|    max|[{'id': 9327, 'lo...|               3.8E8|             180.0|Wolves do not eat...|      2.787965087E9|       시크릿 Secret|\n",
      "+-------+--------------------+--------------------+------------------+--------------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mani.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique/Distinct Values and Counts\n",
    "You may sometimes just want to know the number of levels (cardinality) within a variable. You can do this using the countDistinct function available in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|41138|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Counts the distinct occurances of titles\n",
    "df_mani.agg(countDistinct(col(\"title\")).alias(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+\n",
      "|title                                        |\n",
      "+---------------------------------------------+\n",
      "|The Corn Is Green                            |\n",
      "|Meet The Browns - The Play                   |\n",
      "|Morenita, El Escandalo                       |\n",
      "|Father Takes a Wife                          |\n",
      "|The Werewolf of Washington                   |\n",
      "|My Wife Is a Gangster                        |\n",
      "|Depeche Mode: Touring the Angel Live in Milan|\n",
      "|A Woman Is a Woman                           |\n",
      "|History Is Made at Night                     |\n",
      "|Colombian Love                               |\n",
      "+---------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Counts the distinct occurances of titles\n",
    "df_mani.select('title').distinct().show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|release_year|count(title)|\n",
      "+------------+------------+\n",
      "|1959        |271         |\n",
      "|1990        |496         |\n",
      "|1975        |365         |\n",
      "|1977        |415         |\n",
      "|1924        |19          |\n",
      "|2003        |1199        |\n",
      "|2007        |1896        |\n",
      "|2018        |4           |\n",
      "|1974        |434         |\n",
      "|2015        |13          |\n",
      "+------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extracting year from the release date\n",
    "df_temp = df_mani.withColumn('release_year', year('release_date'))\n",
    "\n",
    "# Extracting month\n",
    "df_temp=df_temp.withColumn('release_month', month('release_date'))\n",
    "\n",
    "# Extracting day of month\n",
    "df_temp=df_temp.withColumn('release_day', dayofmonth('release_date'))\n",
    "\n",
    "# Calculating the distinct counts by the year\n",
    "df_temp.groupBy(\"release_year\").agg(countDistinct(\"title\")).show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----------+------------+-----------+--------------------------+\n",
      "|id   |budget   |popularity|release_date|revenue    |title                     |\n",
      "+-----+---------+----------+------------+-----------+--------------------------+\n",
      "|43957|500000.0 |2.649     |2005-06-28  |1000000.0  |Meet The Browns - The Play|\n",
      "|39997|0.0      |3.585     |1989-11-15  |0.0        |Meet the Hollowheads      |\n",
      "|16710|0.0      |11.495    |2008-03-21  |4.1939392E7|Meet the Browns           |\n",
      "|20430|0.0      |3.614     |2004-01-29  |0.0        |Meet Market               |\n",
      "|76435|0.0      |1.775     |2011-03-31  |0.0        |Meet the In-Laws          |\n",
      "|76516|5000000.0|4.05      |1990-11-08  |485772.0   |Meet the Applegates       |\n",
      "|7278 |3.0E7    |11.116    |2008-01-24  |8.4646832E7|Meet the Spartans         |\n",
      "|32574|0.0      |7.42      |1941-03-14  |0.0        |Meet John Doe             |\n",
      "|40506|0.0      |4.814     |1997-01-31  |0.0        |Meet Wally Sparks         |\n",
      "|40688|2.4E7    |6.848     |1998-03-27  |4562146.0  |Meet the Deedles          |\n",
      "+-----+---------+----------+------------+-----------+--------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter all the titles that start with “Meet”\n",
    "df_mani.filter(df_mani['title'].like('Meet%')).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+----------+------------+-------+---------------------------------------+\n",
      "|id   |budget  |popularity|release_date|revenue|title                                  |\n",
      "+-----+--------+----------+------------+-------+---------------------------------------+\n",
      "|43000|0.0     |2.503     |1962-05-23  |0.0    |The Elusive Corporal                   |\n",
      "|43001|0.0     |5.51      |1962-11-12  |0.0    |Sundays and Cybele                     |\n",
      "|43002|0.0     |5.62      |1962-05-24  |0.0    |Lonely Are the Brave                   |\n",
      "|43003|0.0     |7.159     |1975-03-12  |0.0    |F for Fake                             |\n",
      "|43004|500000.0|3.988     |1962-10-09  |0.0    |Long Day's Journey Into Night          |\n",
      "|43006|0.0     |3.194     |1962-03-09  |0.0    |My Geisha                              |\n",
      "|43007|0.0     |2.689     |1962-10-31  |0.0    |Period of Adjustment                   |\n",
      "|43008|0.0     |6.537     |1959-03-13  |0.0    |The Hanging Tree                       |\n",
      "|43010|0.0     |4.297     |1962-01-01  |0.0    |Sherlock Holmes and the Deadly Necklace|\n",
      "|43011|0.0     |4.417     |1962-01-01  |0.0    |Sodom and Gomorrah                     |\n",
      "+-----+--------+----------+------------+-------+---------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find out the titles that do not end with an “s”\n",
    "df_mani.filter(~df_mani['title'].like('%s')).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+------------+------------+------------------------+\n",
      "|id   |budget|popularity|release_date|revenue     |title                   |\n",
      "+-----+------+----------+------------+------------+------------------------+\n",
      "|43100|0.0   |7.252     |1959-10-07  |0.0         |General Della Rovere    |\n",
      "|43152|0.0   |5.126     |2001-06-21  |0.0         |Love on a Diet          |\n",
      "|43191|0.0   |4.921     |1952-08-29  |0.0         |Beware, My Lovely       |\n",
      "|43281|0.0   |2.411     |1989-11-22  |0.0         |Love Without Pity       |\n",
      "|43343|0.0   |3.174     |1953-12-25  |0.0         |Easy to Love            |\n",
      "|43347|3.0E7 |14.863    |2010-11-22  |1.02820008E8|Love & Other Drugs      |\n",
      "|43362|0.0   |1.705     |1952-02-23  |0.0         |Love Is Better Than Ever|\n",
      "|43363|0.0   |2.02      |1952-05-29  |0.0         |Lovely to Look At       |\n",
      "|43395|0.0   |4.758     |1950-11-10  |0.0         |Two Weeks with Love     |\n",
      "|43455|0.0   |4.669     |1948-08-23  |0.0         |The Loves of Carmen     |\n",
      "+-----+------+----------+------------+------------+------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find any title that contains “ove,” we could use the rlike function, which is a regular expression\n",
    "df_mani.filter(df_mani['title'].rlike('\\w*ove')).show(10,False)\n",
    "# df_mani.filter(df_mani.title.contains('ove')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting and Renaming Columns\n",
    "You can always drop any column or columns using the drop function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop=['budget_cat']\n",
    "\n",
    "# df_with_newcols = df_with_newcols.drop(*columns_to_drop)\n",
    "# df_with_newcols = df_with_newcols.withColumnRenamed('id','film_id') .withColumnRenamed('ratings','film_ratings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to change multiple column names, you try the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('budget', 'film_budget') ('popularity', 'film_popularity')\n"
     ]
    }
   ],
   "source": [
    "new_names = [('budget','film_budget'),('popularity','film_popularity')]\n",
    "\n",
    "print(*zip(*new_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the alias function\n",
    "df_with_newcols_renamed = df_with_newcols.select(list(map(lambda old,new: col(old).alias(new), *zip(*new_names))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('spark38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9cc7abf6a71b1e36bffb4894f1eb166079ff0aa51aabb43b5623fbc056acdf8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
