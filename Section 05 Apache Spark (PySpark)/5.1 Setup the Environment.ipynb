{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Installing Anaconda</h1>\n",
    "\n",
    "<h2>On Linux</h2>\n",
    "\n",
    "We have already disussed in Section 00.\n",
    "\n",
    "<h2>On Windows</h2>\n",
    "\n",
    "The first step is to download Anaconda here: https://www.anaconda.com/. Individuals can download the individual edition (free of cost). I recommend using (Python 3.8) since we use this version in all our examples. If you have an alternate version installed, please make sure to change your code accordingly. Once you install Anaconda, you can run the following command in the Terminal/Anaconda Prompt to ensure the installation is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "conda info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create Conda Environment</h2>\n",
    "\n",
    "Similir to what we have done in Section 00, you can use the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# Activate the base environment\n",
    "conda activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "conda create -–name spark38 python=3.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# List available anaconda environments\n",
    "conda env list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Download and Unpack Apache Spark</h2>\n",
    "\n",
    "You can download the latest version of Apache Spark at https://spark.apache.org/ downloads.html. We are going to use Spark version 3.2 with Hadoop 2.7\n",
    "\n",
    "<h3>On Linux</h3>\n",
    "\n",
    "<li>Unzip the Spark File</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# tar -xzf spark-3.0.1-bin-hadoop2.7.tgz\n",
    "# mv spark-3.0.1-bin-hadoop2.7 /opt/spark-3.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Set Environment Variables</li>\n",
    "\n",
    "Now, we must update the `~/.bash_profile` or `~/.bashrc` files to find the Spark installation. Let us use the bash_profile file in our\n",
    "example here. In Terminal, type the following code to open the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "vi ~/.bash_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the following lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "export SPARK_HOME=/opt/spark-<version>\n",
    "export PATH=$SPARK_HOME/bin:$PATH\n",
    "export PYSPARK_PYTHON=python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "export PYSPARK_DRIVER_PYTHON=jupyter\n",
    "export PYSPARK_DRIVER_PYTHON_OPTS='notebook'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going forward, whenever you invoke pyspark, it should automatically invoke a Jupyter Notebook session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "source ~/.bash_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>On Windows</h3>\n",
    "\n",
    "<li>Unzip the Spark File</li>\n",
    "\n",
    "Extract the downloaded `*.tgz` file to the path `C:\\Users\\<username>\\spark-<version>`. Create `hadoop\\bin` under the SPARK_HOME path, Once the directory is created, we need to copy the winutils.exe file from the our repository under `training-big-data/winutils/hadoop-2.7/` and add it to our Spark installation\n",
    "\n",
    "<li>Set Environment Variables</li>\n",
    "\n",
    "Search for `Environment Variables` in Windows search or you can find them in `System Properties ➤ Advanced ➤ Environment Variables`. Here, we will add a few variables in the User variables tab."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Variable name: SPARK_HOME\n",
    "Variable value: C:\\Users\\username\\spark-3.0.1\n",
    "----------------------------------------------\n",
    "Variable name: HADOOP_HOME\n",
    "Variable value: C:\\Users\\username\\spark-3.0.1\\hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then Add the following value `C:\\Users\\<username>\\spark-<version>\\bin` to the end of the path variable.\n",
    "\n",
    "In your anaconda evironment download `findspark` package which is used in intial the necessary values in order to run `PySprak` on Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "conda install findsaprk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the use it on your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "# If you know spark path you can specify it as init function parameter\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or replace `findspark` code with the below code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "\n",
    "# os.environ[\"PYSPARK_PYTHON\"] = r\"c:\\Users\\dm\\anaconda3\\envs\\spark38\\python.exe\"\n",
    "# os.environ[\"SPARK_HOME\"] = r\"C:\\Users\\dm\\spark-3.3.1\"\n",
    "# os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + r\"\\python\\lib\"\n",
    "# sys.path.insert(0, os.environ[\"PYLIB\"] + r\"\\py4j-0.10.9.5-src.zip\")\n",
    "# sys.path.insert(0, os.environ[\"PYLIB\"] + r\"\\pyspark.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|hello|\n",
      "+-----+\n",
      "|spark|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = spark.sql(\"select 'spark' as hello \")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Spark’s Directories and Files</h2>\n",
    "\n",
    "Let’s briefly summarize the intent and purpose of some of these files and directories.\n",
    "\n",
    "- <b>README.md</b><br />\n",
    "    This file contains new detailed instructions on how to use Spark shells, build Spark from source, run standalone Spark examples, peruse links to Spark documentation and configuration guides, and contribute to Spark.\n",
    "- <b>bin</b><br />\n",
    "    This directory, as the name suggests, contains most of the scripts you’ll employ to interact with Spark, including the Spark shells (spark-sql, pyspark, sparkshell, and sparkR). You will use these shells and executables in this directory later to submit a standalone Spark application using sparksubmit, and write a script that builds and pushes Docker images when running Spark with Kubernetes support.\n",
    "- <b>sbin</b><br />\n",
    "    Most of the scripts in this directory are administrative in purpose, for starting and stopping Spark components in the cluster.\n",
    "- <b>kubernetes</b><br />\n",
    "    Since the release of Spark 2.4, this directory contains Dockerfiles for creating Docker images for your Spark distribution on a Kubernetes cluster. It also contains a file providing instructions on how to build the Spark distribution before building your Docker images.\n",
    "- <b>data</b><br />\n",
    "    This directory is populated with *.txt files that serve as input for Spark’s components: MLlib, Structured Streaming, and GraphX.\n",
    "- <b>examples</b><br />\n",
    "    For any developer, two imperatives that ease the journey to learning any new platform are loads of “how-to” code examples and comprehensive documentation. Spark provides examples for Java, Python, R, and Scala, and you’ll want to employ them when learning the framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Understanding Spark Application Concepts</h2>\n",
    "\n",
    "To understand what’s happening under the hood with our sample code, you’ll need to be familiar with some of the key concepts of a Spark application and how the code is transformed and executed as tasks across the Spark executors. We’ll begin by defining some important terms:\n",
    "\n",
    "- <b>Application</b><br />\n",
    "    A user program built on Spark using its APIs. It consists of a driver program and executors on the cluster.\n",
    "- <b>SparkSession</b><br />\n",
    "    An object that provides a point of entry to interact with underlying Spark functionality and allows programming Spark with its APIs. In an interactive Spark shell, the Spark driver instantiates a SparkSession for you, while in a Spark application, you create a SparkSession oject yourself.\n",
    "- <b>Job</b><br />\n",
    "    A parallel computation consisting of multiple tasks that gets spawned in response to a Spark action (e.g., save(), collect()).\n",
    "- <b>Stage</b><br />\n",
    "    Each job gets divided into smaller sets of tasks called stages that depend on each other.\n",
    "- <b>Task</b><br />\n",
    "    A single unit of work or execution that will be sent to a Spark executor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Extencive Example</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+\n",
      "|State|Color |Total|\n",
      "+-----+------+-----+\n",
      "|CA   |Yellow|1807 |\n",
      "|WA   |Green |1779 |\n",
      "|OR   |Orange|1743 |\n",
      "|TX   |Green |1737 |\n",
      "|TX   |Red   |1725 |\n",
      "|CA   |Green |1723 |\n",
      "|CO   |Yellow|1721 |\n",
      "|CA   |Brown |1718 |\n",
      "|CO   |Green |1713 |\n",
      "|NV   |Orange|1712 |\n",
      "|TX   |Yellow|1703 |\n",
      "|NV   |Green |1698 |\n",
      "|AZ   |Brown |1698 |\n",
      "|WY   |Green |1695 |\n",
      "|CO   |Blue  |1695 |\n",
      "|NM   |Red   |1690 |\n",
      "|AZ   |Orange|1689 |\n",
      "|NM   |Yellow|1688 |\n",
      "|NM   |Brown |1687 |\n",
      "|UT   |Orange|1684 |\n",
      "|NM   |Green |1682 |\n",
      "|UT   |Red   |1680 |\n",
      "|AZ   |Green |1676 |\n",
      "|NV   |Yellow|1675 |\n",
      "|NV   |Blue  |1673 |\n",
      "|WA   |Red   |1671 |\n",
      "|WY   |Red   |1670 |\n",
      "|WA   |Brown |1669 |\n",
      "|NM   |Orange|1665 |\n",
      "|WY   |Blue  |1664 |\n",
      "|WA   |Yellow|1663 |\n",
      "|WA   |Orange|1658 |\n",
      "|CA   |Orange|1657 |\n",
      "|NV   |Brown |1657 |\n",
      "|CA   |Red   |1656 |\n",
      "|CO   |Brown |1656 |\n",
      "|UT   |Blue  |1655 |\n",
      "|AZ   |Yellow|1654 |\n",
      "|TX   |Orange|1652 |\n",
      "|AZ   |Red   |1648 |\n",
      "|OR   |Blue  |1646 |\n",
      "|UT   |Yellow|1645 |\n",
      "|OR   |Red   |1645 |\n",
      "|CO   |Orange|1642 |\n",
      "|TX   |Brown |1641 |\n",
      "|NM   |Blue  |1638 |\n",
      "|AZ   |Blue  |1636 |\n",
      "|OR   |Green |1634 |\n",
      "|UT   |Brown |1631 |\n",
      "|WY   |Yellow|1626 |\n",
      "|WA   |Blue  |1625 |\n",
      "|CO   |Red   |1624 |\n",
      "|OR   |Brown |1621 |\n",
      "|TX   |Blue  |1614 |\n",
      "|OR   |Yellow|1614 |\n",
      "|NV   |Red   |1610 |\n",
      "|CA   |Blue  |1603 |\n",
      "|WY   |Orange|1595 |\n",
      "|UT   |Green |1591 |\n",
      "|WY   |Brown |1532 |\n",
      "+-----+------+-----+\n",
      "\n",
      "Total Rows = 60\n",
      "+-----+------+-----+\n",
      "|State|Color |Total|\n",
      "+-----+------+-----+\n",
      "|CA   |Yellow|1807 |\n",
      "|CA   |Green |1723 |\n",
      "|CA   |Brown |1718 |\n",
      "|CA   |Orange|1657 |\n",
      "|CA   |Red   |1656 |\n",
      "|CA   |Blue  |1603 |\n",
      "+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Build a SparkSession using the SparkSession APIs.\n",
    "    # If one does not exist, then create an instance. There\n",
    "    # can only be one SparkSession per JVM.\n",
    "    spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"PythonMnMCount\")\n",
    "    .getOrCreate())\n",
    "\n",
    "    mnm_file = \"../data/mnm_dataset.csv\"\n",
    "\n",
    "    # Read the file into a Spark DataFrame using the CSV\n",
    "    # format by inferring the schema and specifying that the\n",
    "    # file contains a header, which provides column names for comma-\n",
    "    # separated fields.\n",
    "    mnm_df = (spark.read.format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(mnm_file))\n",
    "\n",
    "    # We use the DataFrame high-level APIs. Note\n",
    "    # that we don't use RDDs at all. Because some of Spark's\n",
    "    # functions return the same object, we can chain function calls.\n",
    "    # 1. Select from the DataFrame the fields \"State\", \"Color\", and \"Count\"\n",
    "    # 2. Since we want to group each state and its M&M color count,\n",
    "    # we use groupBy()\n",
    "    # 3. Aggregate counts of all colors and groupBy() State and Color\n",
    "    # 4 orderBy() in descending order\n",
    "    count_mnm_df = (mnm_df\n",
    "        .select(\"State\", \"Color\", \"Count\")\n",
    "        .groupBy(\"State\", \"Color\")\n",
    "        .agg(count(\"Count\").alias(\"Total\"))\n",
    "        .orderBy(\"Total\", ascending=False))\n",
    "    \n",
    "    # Show the resulting aggregations for all the states and colors;\n",
    "    # a total count of each color per state.\n",
    "    # Note show() is an action, which will trigger the above\n",
    "    # query to be executed.\n",
    "    count_mnm_df.show(n=60, truncate=False)\n",
    "    \n",
    "    print(\"Total Rows = %d\" % (count_mnm_df.count()))\n",
    "    \n",
    "    # While the above code aggregated and counted for all\n",
    "    # the states, what if we just want to see the data for\n",
    "    # a single state, e.g., CA?\n",
    "    # 1. Select from all rows in the DataFrame\n",
    "    # 2. Filter only CA state\n",
    "    # 3. groupBy() State and Color as we did above\n",
    "    # 4. Aggregate the counts for each color\n",
    "    # 5. orderBy() in descending order\n",
    "    # Find the aggregate count for California by filtering\n",
    "    ca_count_mnm_df = (mnm_df\n",
    "    .select(\"State\", \"Color\", \"Count\")\n",
    "    .where(mnm_df.State == \"CA\")\n",
    "    .groupBy(\"State\", \"Color\")\n",
    "    .agg(count(\"Count\").alias(\"Total\"))\n",
    "    .orderBy(\"Total\", ascending=False))\n",
    "\n",
    "    # Show the resulting aggregation for California.\n",
    "    # As above, show() is an action that will trigger the execution of the\n",
    "    # entire computation.\n",
    "    ca_count_mnm_df.show(n=10, truncate=False)\n",
    "\n",
    "    # Stop the SparkSession\n",
    "    spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec3a724ff662ede54089d8c0bc916d65e9da1859882934a442dfe598498fa369"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
