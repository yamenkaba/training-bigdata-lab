{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing and Tuning Spark Applications\n",
    "## Optimizing and Tuning Spark for Efficiency\n",
    "In this chapter will only cover a handful of the most important and commonly tuned configurations. For a comprehensive list grouped by functional themes, you can peruse the <a href=\"https://spark.apache.org/docs/latest/configuration.html\">documentation</a>.\n",
    "\n",
    "### Viewing and Setting Apache Spark Configurations\n",
    "There are three ways you can get and set Spark properties. The first is through a set of configuration files. In your deployment’s directory\n",
    "`$SPARK_HOME`, there are a number of config files: `conf/spark-defaults.conf.template`, `conf/log4j.properties.template`, and `conf/spark-env.sh.template`. Changing the default values in these files and saving them without the .template suffix instructs Spark to use these new values.\n",
    "\n",
    "The second way is to specify Spark configurations directly in your Spark application or on the command line when submitting the application with spark-submit, using the --conf flag:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "spark-submit --conf spark.sql.shuffle.partitions=5 --conf \"spark.executor.memory=2g\" --class main.scala.chapter7.SparkConfig_7_1 jars/main scala-\n",
    "chapter7_2.12-1.0.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s how you would do this in the Spark application itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "# If you know spark path you can specify it as init function parameter\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = (SparkSession\n",
    "            .builder\n",
    "            .appName(\"SparkOptimizingTuning\")\n",
    "            .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.app.startTime', '1671362818086'),\n",
       " ('spark.driver.host', 'yamen'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.submitTime', '1671362817924'),\n",
       " ('spark.app.id', 'local-1671362820512'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.app.name', 'SparkOptimizingTuning'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.port', '65145'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.app.name\", \"SparkOptimizingAndTuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SparkOptimizingAndTuning'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.app.name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among all the ways that you can set Spark properties, an order of precedence determines which values are honored. Any values or flags defined in `spark-defaults.conf` will be read first, followed by those supplied on the command line with sparksubmit, and finally those set via  `SparkSession` in the Spark application. All these properties will be merged, with any duplicate properties reset in the Spark application\n",
    "taking precedence. Likewise, values supplied on the command line will supersede settings in the configuration file, provided they are not overwritten in the application itself.\n",
    "\n",
    "### Scaling Spark for Large Workloads\n",
    "Large Spark workloads are often batch jobs—some run on a nightly basis, while some are scheduled at regular intervals during the day. In either case, these jobs may process tens of terabytes of data or more.To avoid job failures due to resource starvation or gradual performance degradation, there are a handful of Spark configurations that you can enable or alter. These configurations affect three Spark components: the\n",
    "Spark driver, the executor, and the shuffle service running on the executor.\n",
    "\n",
    "The Spark driver’s responsibility is to coordinate with the cluster manager to launch executors in a cluster and schedule Spark tasks on them. With large workloads, you may have hundreds of tasks. This section explains a few configurations you can tweak or enable to optimize your resource utilization, parallelize tasks, and avoid bottlenecks for large numbers of tasks.\n",
    "\n",
    "#### Static versus dynamic resource allocation\n",
    "When you specify compute resources as command-line arguments to spark-submit, as we did earlier, you cap the limit. This means that if more resources are needed later as tasks queue up in the driver due to a larger than anticipated workload, Spark cannot accommodate or allocate extra resources.\n",
    "\n",
    "If instead you use Spark’s dynamic resource allocation configuration, the Spark driver can request more or fewer compute resources as the demand of large workloads flows and ebbs. In scenarios where your workloads are dynamic—that is, they vary in their demand for compute capacity—using dynamic allocation helps to accommodate sudden peaks.\n",
    "\n",
    "One use case where this can be helpful is streaming, where the data flow volume may be uneven. Another is on-demand data analytics, where you might have a high volume of SQL queries during peak hours. Enabling dynamic resource allocation allows Spark to achieve better utilization of resources, freeing executors when not in use and acquiring new ones when needed.\n",
    "\n",
    "To enable and configure dynamic allocation, you can use settings like the following. Note that the numbers here are arbitrary; the appropriate settings will depend on the nature of your workload and they should be adjusted accordingly. Some of these configs cannot be set inside a Spark REPL(SHELL), so you will have to set them programmatically:\n",
    "\n",
    "spark.dynamicAllocation.enabled true\n",
    "spark.dynamicAllocation.minExecutors 2\n",
    "spark.dynamicAllocation.schedulerBacklogTimeout 1m\n",
    "spark.dynamicAllocation.maxExecutors 20\n",
    "spark.dynamicAllocation.executorIdleTimeout 2min\n",
    "\n",
    "By default `spark.dynamicAllocation.enabled` is set to `false`. When enabled with the settings shown here, the Spark driver will request that the cluster manager create two executors to start with, as a minimum `(spark.dynamicAllocation.minExecutors)`. As the task queue backlog increases, new executors will be requested each time the backlog timeout `(spark.dynamicAllocation.schedulerBacklogTimeout)` is exceeded. In this case, whenever there are pending tasks that have not been scheduled for over 1 minute, the driver will request that a new executor be launched to schedule backlogged tasks, up to a maximum of 20 `(spark.dynamicAllocation.maxExecutors)`. By contrast, if an executor finishes a task and is idle for 2 minutes `(spark.dynamicAllocation.executorIdleTimeout)`, the Spark driver will terminate it.\n",
    "\n",
    "#### Configuring Spark executors’ memory and the shuffle service\n",
    "Simply enabling dynamic resource allocation is not sufficient. You also have to understand how executor memory is laid out and used by Spark so that executors are not starved of memory or troubled by JVM garbage collection.\n",
    "\n",
    "The amount of memory available to each executor is controlled by `spark.executor.memory`. This is divided into three sections, `execution memory`, `storage memory`, and `reserved memor`y. The default division is `60% for execution memory` and `40% for storage`, after allowing for `300 MB for reserved memory`, to safeguard against OOM errors.\n",
    "The Spark documentation advises that this will work for most cases, but you can adjust what fraction of `spark.executor.memory` you want either section to use as a baseline. When storage memory is not being used, Spark can acquire it for use in execution memory for execution purposes, and vice versa.\n",
    "Execution memory is used for Spark shuffles, joins, sorts, and aggregations. Since different queries may require different amounts of memory, the fraction `(spark.memory.fraction is 0.6 by default)` of the available memory to dedicate to this can be tricky to tune but it’s easy to adjust. By contrast, storage memory is primarily used for caching user data structures and partitions derived from DataFrames.\n",
    "\n",
    "In the below table, we capture a few recommended configurations to adjust so that the map, split, and merge processes during these operations are not encumbered by inefficient I/O and to enable these operations to employ buffer memory before writing the final shuffle partitions to disk. Tuning the shuffle service running on each executor can also aid in increasing overall performance for large Spark workloads.\n",
    "\n",
    "| Configuration      | Default value, recommendation, and description |\n",
    "| :----:      |    :----   |\n",
    "| spark.driver.memory      | Default is 1g (1 GB). This is the amount of memory allocated to the Spark driver to receive data from executors. This is often changed during sparksubmit with --driver-memory. Only change this if you expect the driver to receive large amounts of data back from operations like collect(), or if you run out of driver memory.       |\n",
    "| spark.shuffle.file.buffer   | Default is 32 KB. Recommended is 1 MB. This allows Spark to do more buffering before writing final map results to disk.        |\n",
    "| spark.file.transferTo   | Default is true. Setting it to false will force Spark to use the file buffer to transfer files before finally writing to disk; this will decrease the I/O activity.        |\n",
    "| spark.shuffle.unsafe.file.output.buffer   | Default is 32 KB. This controls the amount of buffering possible when merging files during shuffle operations. In general, large values (e.g., 1 MB) are more appropriate for larger workloads, whereas the default can work for smaller workloads.        |\n",
    "| spark.io.compression.lz4.blockSize   | Default is 32 KB. Increase to 512 KB. You can decrease the size of the shuffle file by increasing the compressed size of the block.        |\n",
    "| spark.shuffle.service.index.cache.size   | Default is 100m. Cache entries are limited to the specified memory footprint in byte.        |\n",
    "| spark.shuffle.registration.timeout   | Default is 5000 ms. Increase to 120000 ms.        |\n",
    "| spark.shuffle.registration.maxAttempts   | Default is 3. Increase to 5 if needed.        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximizing Spark parallelism\n",
    "Much of Spark’s efficiency is due to its ability to run multiple tasks in parallel at scale. To understand how you can maximize parallelism     i.e., read and process as much data in parallel as possible—you have to look into how Spark reads data into memory from storage and what partitions mean to Spark.\n",
    "In data management parlance, a partition is a way to arrange data into a subset of configurable and readable chunks or blocks of contiguous data on disk. These subsets of data can be read or processed independently and in parallel, if necessary, by more than a single thread in a process. This independence matters because it allows for massive parallelism of data processing.\n",
    "\n",
    "### How partitions are created\n",
    "Spark’s tasks process data as partitions read from disk into memory. Data on disk is laid out in chunks or contiguous file blocks, depending on the store. By default, file blocks on data stores range in size from 64 MB to 128 MB. For example, on HDFS and S3 the default size is 128 MB\n",
    "(this is configurable). A contiguous collection of these blocks constitutes a partition. The size of a partition in Spark is dictated by spark.`sql.files.maxPartitionBytes`. The default is 128 MB.\n",
    "\n",
    "Partitions are also created when you explicitly use certain methods of the DataFrame API. For example, while creating a large DataFrame or reading a large file from disk, you can explicitly instruct Spark to create a certain number of partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method RDD.getNumPartitions of MapPartitionsRDD[8] at javaToPython at NativeMethodAccessorImpl.java:0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numDF = spark.range(1000 * 1000).repartition(8)\n",
    "numDF.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, shuffle partitions are created during the shuffle stage. By default, the number of shuffle partitions is set to 200 in `spark.sql.shuffle.partitions`. You can adjust this number depending on the size of the data set you have, to reduce the amount of small partitions being sent across the network to executors’ tasks.\n",
    "\n",
    "<b>Note!</b> The default value for `spark.sql.shuffle.partitions` is too high for smaller or streaming workloads; you may want to reduce it to a\n",
    "lower value such as the number of cores on the executors or less.\n",
    "\n",
    "Created during operations like groupBy() or join(), also known as wide transformations, shuffle partitions consume both network and disk I/O resources. During these operations, the shuffle will spill results to executors’ local disks at the location specified in `spark.local.directory`. Having performant SSD disks for this operation will boost the performance.\n",
    "\n",
    "There is no magic formula for the number of shuffle partitions to set for the shuffle stage; the number may vary depending on your use case, data set, number of cores, and the amount of executor memory available—it’s a trial-and-error approach.\n",
    "\n",
    "<b>Note!</b> Check the follwing tow links for more information <a href=\"https://www.youtube.com/watch?v=5dga0UT4RI8&ab_channel=Databricks\">Tuning Apache Spark for Large Scale Workloads</a>, <a href=\"https://www.youtube.com/watch?v=6BD-Vv-ViBw&t=645s&ab_channel=Databricks\">Hive Bucketing in Apache Spark</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching and Persistence of Data\n",
    "What is the difference between caching and persistence? In Spark they are synonymous. Two API calls, `cache()` and `persist()`, offer these  capabilities. The latter provide more control over how and where your data is stored—in memory and on disk, serialized and unserialized. Both contribute to better performance for frequently accessed DataFrames or tables.\n",
    "\n",
    "### DataFrame.cache()\n",
    "`cache()` will store as many of the partitions read in memory across Spark executors as memory allows. While a DataFrame may be fractionally cached, partitions cannot be fractionally cached (e.g., if you have 8 partitions but only 4.5 partitions can fit in memory, only 4 will be cached). However, if not all your partitions are cached, when you want to access the data again, the partitions that are not cached will have to be recomputed, slowing down your Spark job.\n",
    "\n",
    "Let’s look at an example of how caching a large DataFrame improves performance when accessing a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = spark.range(1 * 10000000).toDF(\"id\").withColumn(\"square\", col(\"id\") * col(\"id\"))\n",
    "df.cache() # Cache the data\n",
    "df.count() # Materialize the cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note!</b> When you use `cache()` or `persist()`, the DataFrame is not fully cached until you invoke an action that goes through every record\n",
    "(e.g., count()). If you use an action like `take(1)`, only one partition will be cached because Catalyst realizes that you do not need to compute all the partitions just to retrieve one record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame.persist()\n",
    "`persist(StorageLevel.LEVEL)` is nuanced, providing control over how your data is cached via StorageLevel. Below table summarizes the different storage levels. Data on disk is always serialized using either Java or Kryo serialization.\n",
    "\n",
    "| StorageLevel      | Description |\n",
    "| :----      |    :----   |\n",
    "| MEMORY_ONLY      | Data is stored directly as objects and stored only in memory.       |\n",
    "| MEMORY_ONLY_SER   | Data is serialized as compact byte array representation and stored only in memory. To use it, it has to be deserialized at a cost.        |\n",
    "| MEMORY_AND_DISK   | Data is stored directly as objects in memory, but if there’s insufficient memory the rest is serialized and stored on disk.        |\n",
    "| DISK_ONLY   | Data is serialized and stored on disk.        |\n",
    "| OFF_HEAP   | Data is stored off-heap. Off-heap memory is used in Spark for storage and query execution.        |\n",
    "| MEMORY_AND_DISK_SER   | Like MEMORY_AND_DISK, but data is serialized when stored in memory. (Data is always serialized when stored on disk.)        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "df2 = spark.range(1 * 20000000).toDF(\"id\").withColumn(\"square\", col(\"id\") * col(\"id\"))\n",
    "df2.persist(StorageLevel.DISK_ONLY) # Serialize the data and cache it on disk\n",
    "df2.count() # Materialize the cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count() # Now get it from the cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note!</b> As you can see in Spark UI under Storage, the data is persisted on disk, not in memory. To unpersist your cached data, just call DataFrame.unpersist()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|10000000|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"dfTable\")\n",
    "\n",
    "spark.sql(\"CACHE TABLE dfTable\")\n",
    "\n",
    "spark.sql(\"SELECT count(*) FROM dfTable\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Cache and Persist\n",
    "Common use cases for caching are scenarios where you will want to access a large data set repeatedly for queries or transformations. Some examples include:\n",
    "- DataFrames commonly used during iterative machine learning training\n",
    "- DataFrames accessed commonly for doing frequent transformations during ETL or building data pipelines\n",
    "\n",
    "### When Not to Cache and Persist\n",
    "Not all use cases dictate the need to cache. Some scenarios that may not warrant caching your DataFrames include:\n",
    "- DataFrames that are too big to fit in memory\n",
    "- An inexpensive transformation on a DataFrame not requiring frequent use, regardless of size\n",
    "\n",
    "As a general rule you should use memory caching judiciously, as it can incur resource costs in serializing and deserializing, depending on the StorageLevel used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('spark38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9cc7abf6a71b1e36bffb4894f1eb166079ff0aa51aabb43b5623fbc056acdf8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
