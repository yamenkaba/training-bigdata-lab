{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory\n",
    "As we saw in the previous sections, as the network grows larger, the RNNs are not able to successfully propagate the gradients that tend to become zero (or in some cases, infinite), thus leading to vanishing gradient problem. This problem is evident in problems where we receive a long sequence as input, for example, long sentences from a product review or a blog post.\n",
    "\n",
    "Consider the following example:\n",
    "\n",
    "    I didn’t go to Amsterdam this year but I was there during the Summer last year and it was quite crowded everywhere in the city.\n",
    "\n",
    "\n",
    "Not all the words are relevant to interpret the meaning of the sentence. But when we look at the whole sentence, it is important to know that the word “city” refers to the same entity that is referred to by the word Amsterdam. Long dependencies, as the ones shown in below figure, should be preserved.\n",
    "\n",
    "In RNN, the input at the current time step and the hidden state computed in the previous time step are combined and passed through tanh activation. The output thus created also acts as the hidden state for the next time step. Long short-term memory, or LSTM, is an architecture that partially solves the vanishing gradient problem by allowing the gradients to flow unchanged. LSTM cells are constructed specifically to remember the parts of the sequence that matter, thus keeping only the relevant information to make prediction.\n",
    "\n",
    "![images/rnn-lst-1.png](images/rnn-lst-1.png)\n",
    "\n",
    "LSTMs are designed to hold a cell state that can be referred to as a memory. Due to a cell holding a memory, information from earlier time steps can be transferred to the later time steps, thus preserving long-term dependencies in sequences. The memory block is controlled by gates – which are simple operations that decide which information should be kept or forgotten during the training. This is implemented by applying sigmoid activation at certain computations within the cell that outputs zero for any negative values provided to it, thus leading to the concept of forgetting the information.\n",
    "\n",
    "## LSTM Cell\n",
    "The structure of an LSTM Cell is shown in below figure. The first section of LSTM cell that receives the input is called a forget gate. Forget gate decides the information that should be kept or forgotten (or discarded). This gate combines the hidden state from the previous time step and the current input and passes them through the sigmoid function.\n",
    "\n",
    "Regardless of the forget operation, the input combined with the previous hidden state is passed through sigmoid function and a tanh function. The output of tanh function acts as the candidate, which may be propagated further, and the output of the sigmoid function acts as an evaluation function that decides the importance of the values. The tanh output is then multiplied with the sigmoid output. This section of the LSTM cell is\n",
    "called the input gate.\n",
    "\n",
    "![images/rnn-cell-structure.png](images/rnn-cell-structure.png)\n",
    "\n",
    "The output of forget gate is a vector, which is multiplied with the previous cell state, thus making the values that should be forgotten zero. We add this to the vector computed by combining the value obtained at the input gate, thus providing a new cell state. The output of the LSTM cell becomes the hidden state, which will be later combined with the input in the next step – meanwhile, the cell state gets propagated without\n",
    "further transformations to the next steps. The output gate combines the candidate (previous hidden state and current input, followed by a sigmoid activation) and the cell state that was just computed, on which a tanh activation is first applied by performing a multiplication that forms the next hidden state."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec3a724ff662ede54089d8c0bc916d65e9da1859882934a442dfe598498fa369"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
